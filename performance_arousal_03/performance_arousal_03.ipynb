{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "31c8e4b7-03f8-4d35-8ea6-7b6c168e86c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyxdf\n",
    "import logging\n",
    "import time\n",
    "import pprint\n",
    "import ast\n",
    "import pytz\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from pison_cloud.pison.reaction.cloud.v1 import reaction_pb2, reaction_pb2_grpc\n",
    "from pison_cloud.pison.common.cloud.v1 import common_pb2\n",
    "from google.protobuf.timestamp_pb2 import Timestamp\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "from ml_util.query.microservices import (\n",
    "    PisonGrpc,\n",
    "    ResponseConverter,\n",
    "    get_users,\n",
    "    get_all_metadata,\n",
    "    get_reaction_tests,\n",
    "    get_plan_data,\n",
    "    get_baseline_data\n",
    ")\n",
    "from ml_util.query.utils import Env\n",
    "\n",
    "from ml_util.pison_ready.readiness import get_score as get_readiness_score\n",
    "from ml_util.pison_ready.agility import get_score as get_agility_score\n",
    "from ml_util.pison_ready.focus import get_score as get_focus_score\n",
    "from ml_util.pison_ready.pvtb import get_score as get_pvtb_score\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import pyxdf\n",
    "\n",
    "def extract_eda_streams(xdf_file_path):\n",
    "    # Load the XDF file\n",
    "    streams, fileheader = pyxdf.load_xdf(xdf_file_path)\n",
    "    \n",
    "    # Extract the reference time from the file header\n",
    "    reference_time_str = fileheader['info']['datetime'][0]\n",
    "    reference_time = datetime.strptime(reference_time_str, '%Y-%m-%dT%H:%M:%S%z')\n",
    "\n",
    "    # Initialize a list to store the dataframes for each EDA stream\n",
    "    eda_data = []\n",
    "\n",
    "    # Loop through streams to find EDA streams\n",
    "    for stream in streams:\n",
    "        if stream['info']['type'][0] == 'EDA':\n",
    "            # Extract data from the stream\n",
    "            timestamps = stream['time_stamps']\n",
    "            values = stream['time_series']\n",
    "            source_id = stream['info']['source_id'][0]\n",
    "            \n",
    "            # Combine the data into a dataframe\n",
    "            data = pd.DataFrame({\n",
    "                'Timestamp': timestamps,\n",
    "                'Value': [value[0] for value in values],  # Assuming a single channel EDA\n",
    "                'SourceID': source_id\n",
    "            })\n",
    "            \n",
    "            # Correct the hardware timestamps using the first timestamp in the dataframe as a reference\n",
    "            reference_hardware_timestamp = float(data['Timestamp'].iloc[0])\n",
    "            data['Converted_Timestamp'] = data['Timestamp'].apply(lambda x: reference_time + timedelta(seconds=(float(x) - reference_hardware_timestamp)))\n",
    "            \n",
    "            # Drop the original 'Timestamp' column and rename 'Converted_Timestamp' to 'Timestamp'\n",
    "            data.drop(columns=['Timestamp'], inplace=True)\n",
    "            data.rename(columns={'Converted_Timestamp': 'Timestamp'}, inplace=True)\n",
    "            \n",
    "            # Append the corrected dataframe to the list\n",
    "            eda_data.append(data)\n",
    "\n",
    "    # Return a combined dataframe for all EDA streams\n",
    "    if len(eda_data) > 0:\n",
    "        return pd.concat(eda_data, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=['Timestamp', 'Value', 'SourceID'])\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "def get_stim_timestamps(df, log_file='skipped_sessions.txt'):\n",
    "    \"\"\"\n",
    "    Calculate the timestamps for each stimulus, the time intervals between consecutive stimuli in seconds,\n",
    "    the stimulus offsets by subtracting 80 milliseconds from each timestamp, map the first timestamp from the list \n",
    "    that is immediately after the stim time offset, compute the delta between onset_timestamp and stim time,\n",
    "    and mark rows as false starts if necessary.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing columns ['plan.stimuli', 'created_at', 'onset_moments', 'session_id']\n",
    "    log_file (str): The file to log skipped sessions due to insufficient timestamps.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with timestamps, intervals between stimuli in seconds, stimulus offsets, mapped timestamps,\n",
    "    delta between onset_timestamp and stim time, false starts, and trial numbers.\n",
    "    \"\"\"\n",
    "    # Extract data from DataFrame\n",
    "    data = df['plan.stimuli'].iloc[0]  # Assuming 'plan.stimuli' is a list of dictionaries in each row\n",
    "    start_time = df['created_at'].iloc[0]  # Assuming 'created_at' is a single datetime-like string or pd.Timestamp\n",
    "    timestamps_list = df['onset_moments'].iloc[0]  # Assuming 'onset_moments' is a list of timestamp strings\n",
    "    session_id = df['session_id'].iloc[0]  # Assuming 'session_id' is a single string\n",
    "\n",
    "    # Convert start time to a datetime object if it's not already\n",
    "    if isinstance(start_time, str):\n",
    "        start_time = datetime.fromisoformat(start_time)\n",
    "    elif isinstance(start_time, pd.Timestamp):\n",
    "        start_time = start_time.to_pydatetime()\n",
    "\n",
    "    # Adjust for the 5-second countdown\n",
    "    countdown_adjustment = timedelta(seconds=0)\n",
    "    adjusted_start_time = start_time + countdown_adjustment\n",
    "\n",
    "    # Check if there are enough timestamps\n",
    "    if len(timestamps_list) < 45:\n",
    "        with open(log_file, 'a') as f:\n",
    "            f.write(f\"Session {session_id} skipped: Less than 45 timestamps.\\n\")\n",
    "        return None\n",
    "\n",
    "    # Calculate the timestamps and intervals\n",
    "    timeInSeconds = [entry['timeInSeconds'] for entry in data]\n",
    "    no_go = [entry.get('noGo', False) for entry in data]\n",
    "    timestamps = [adjusted_start_time + timedelta(seconds=entry['timeInSeconds']) for entry in data]\n",
    "    stim_offsets = [timestamp - timedelta(milliseconds=80) for timestamp in timestamps]\n",
    "    intervals = [timestamps[i] - timestamps[i-1] for i in range(1, len(timestamps))]\n",
    "    intervals_in_seconds = [interval.total_seconds() for interval in intervals] + [None]\n",
    "\n",
    "    # Parse the provided list of timestamps into datetime objects\n",
    "    parsed_timestamps_list = [datetime.fromisoformat(ts.replace('Z', '+00:00')) for ts in timestamps_list]\n",
    "\n",
    "    # Find the first timestamp from the list that is immediately after the stim time offset\n",
    "    mapped_timestamps = []\n",
    "    for i, stim_offset in enumerate(stim_offsets):\n",
    "        next_stim_time = timestamps[i + 1] if i + 1 < len(timestamps) else timestamps[i] + timedelta(days=1)\n",
    "        next_timestamp = next((ts for ts in parsed_timestamps_list if stim_offset < ts < next_stim_time), None)\n",
    "        mapped_timestamps.append(next_timestamp)\n",
    "\n",
    "    # Calculate the delta between onset_timestamp and stim time\n",
    "    delta_onset_stim = [(mapped - stim).total_seconds() if pd.notna(mapped) else pd.NaT \n",
    "                        for mapped, stim in zip(mapped_timestamps, timestamps)]\n",
    "\n",
    "    # Mark rows as false starts if delta_onset_stim is < 0.08\n",
    "    false_starts = [(delta < 0.08) if pd.notna(delta) else False for delta in delta_onset_stim]\n",
    "\n",
    "    # Create trial numbers starting from 1\n",
    "    trials = list(range(1, len(timeInSeconds) + 1))\n",
    "\n",
    "    # Create DataFrame\n",
    "    result_df = pd.DataFrame({\n",
    "        'trial': trials,\n",
    "        'timeInSeconds': timeInSeconds,\n",
    "        'stim time': timestamps,\n",
    "        'stim time offset (-80ms)': stim_offsets,\n",
    "        'ISI (seconds)': intervals_in_seconds,\n",
    "        'start_of_test': start_time,\n",
    "        'onset_timestamp': mapped_timestamps,\n",
    "        'delta_onset_stim (seconds)': delta_onset_stim,\n",
    "        'false start': false_starts,\n",
    "        'noGo': no_go,\n",
    "        'session_id': session_id\n",
    "    })\n",
    "\n",
    "    return result_df\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def merge_eda_with_stim(single_test_df, eda_data_df):\n",
    "    \"\"\"\n",
    "    Merges EDA data from two sensors with single test data based on the closest timestamp using merge_asof.\n",
    "    \n",
    "    Parameters:\n",
    "    single_test_df (pd.DataFrame): DataFrame containing single test data with 'stim time' in UTC.\n",
    "    eda_data_df (pd.DataFrame): DataFrame containing EDA data with 'Timestamp' in EST and 'SourceID'.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: A merged DataFrame with the closest EDA data matched to each stim time for both sensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert 'stim time' from UTC to EST for accurate matching\n",
    "    single_test_df['stim time'] = pd.to_datetime(single_test_df['stim time'], utc=True).dt.tz_convert('US/Eastern')\n",
    "    \n",
    "    # Convert EDA timestamps to datetime if not already and ensure they are in the same timezone (US/Eastern)\n",
    "    eda_data_df['Timestamp'] = pd.to_datetime(eda_data_df['Timestamp']).dt.tz_convert('US/Eastern')\n",
    "    \n",
    "    # Separate EDA data for each sensor\n",
    "    eda_319 = eda_data_df[eda_data_df['SourceID'] == 'MD-V5-0000319'].sort_values('Timestamp')\n",
    "    eda_395 = eda_data_df[eda_data_df['SourceID'] == 'MD-V5-0000395'].sort_values('Timestamp')\n",
    "    \n",
    "    # Perform asof merge to find the closest EDA record before or after the stim time for each sensor\n",
    "    result_319 = pd.merge_asof(single_test_df.sort_values('stim time'), eda_319, \n",
    "                               left_on='stim time', right_on='Timestamp', \n",
    "                               direction='nearest', suffixes=('', '_319'))\n",
    "    \n",
    "    result_395 = pd.merge_asof(single_test_df.sort_values('stim time'), eda_395, \n",
    "                               left_on='stim time', right_on='Timestamp', \n",
    "                               direction='nearest', suffixes=('', '_395'))\n",
    "    \n",
    "    # Merge the two results to include both sensors in the final output\n",
    "    final_result = result_319.merge(\n",
    "        result_395[['stim time', 'Timestamp', 'Value']],\n",
    "        on='stim time',\n",
    "        suffixes=('_319', '_395'),\n",
    "        how='left'\n",
    "    ).rename(columns={'Timestamp_319': 'Timestamp_319', 'Value_319': 'EDA_Value_319',\n",
    "                      'Timestamp_395': 'Timestamp_395', 'Value_395': 'EDA_Value_395'})\n",
    "\n",
    "    return final_result\n",
    "\n",
    "\n",
    "def align_hrv_to_test(single_test_df, hrv_df):\n",
    "    # Convert HRV 'Date' from EST to UTC\n",
    "    hrv_df['Date'] = pd.to_datetime(hrv_df['Date'])\n",
    "    hrv_df['Date_UTC'] = hrv_df['Date'].dt.tz_localize('America/New_York').dt.tz_convert('UTC')\n",
    "\n",
    "    # Convert 'start_of_test' and 'onset_timestamp' in single_test_df to datetime\n",
    "    single_test_df['start_of_test'] = pd.to_datetime(single_test_df['start_of_test'])\n",
    "    single_test_df['onset_timestamp'] = pd.to_datetime(single_test_df['onset_timestamp'])\n",
    "\n",
    "    # Initialize columns for HRV data in single_test_df\n",
    "    hrv_columns = hrv_df.columns.drop(['Date_UTC'])  # Keep 'Date' column\n",
    "\n",
    "    for col in hrv_columns:\n",
    "        single_test_df[f'HRV_{col}'] = None\n",
    "\n",
    "    # Iterate through each row in the single test dataframe\n",
    "    for index, test_row in single_test_df.iterrows():\n",
    "        # Filter HRV rows by the same date as 'start_of_test'\n",
    "        same_date_hrv = hrv_df[hrv_df['Date_UTC'].dt.date == test_row['start_of_test'].date()]\n",
    "        \n",
    "        if not same_date_hrv.empty:\n",
    "            # Find the closest HRV 'Date_UTC' to 'onset_timestamp'\n",
    "            closest_hrv_row = same_date_hrv.iloc[(same_date_hrv['Date_UTC'] - test_row['onset_timestamp']).abs().argsort()[:1]]\n",
    "            \n",
    "            # Update single_test_df with the closest HRV row data\n",
    "            for col in hrv_columns:\n",
    "                single_test_df.at[index, f'HRV_{col}'] = closest_hrv_row[col].values[0]\n",
    "\n",
    "    return single_test_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1923ef71-5947-408e-aee6-b1e39ff06e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv from Wellatory \n",
    "hrv_data = pd.read_csv('HRV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7965e523-3c9d-4397-9d04-36a0057c6a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xdfs for EDA data\n",
    "file_path = '' \n",
    "eda_streams_dataframes = extract_eda_streams(file_path)\n",
    "eda_streams_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6843b97d-0701-4ba6-8ce3-c65207f4a8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = dt.datetime(2024, 8, 1, 0, 0, 0)\n",
    "end_date = dt.datetime(2024, 8, 30, 0, 0, 0)\n",
    "env = Env.STAGING\n",
    "user_df = get_users(env)\n",
    "user_df = user_df.drop(columns='created_at')\n",
    "\n",
    "test_df = get_reaction_tests(env, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373b20b4-6359-4381-bb4d-4844db14183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get mikoshilab@pison.com sessions if no session_id and match via timestmaps\n",
    "test_df[test_df['user_id']=='zJX3F0VX7EXwIHLN5Vca0wog2yf2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "047fbe2d-fea7-4cb7-9718-ffd3919f9377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the DataFrame is test_df and the column with session_id is named 'session_id'\n",
    "partial_id = \"210d0605\"  # Replace this with your actual partial ID string\n",
    "\n",
    "# Filter the DataFrame for rows where 'session_id' contains the partial ID\n",
    "matching_rows = test_df[test_df['session_id'].str.contains(partial_id, na=False)]\n",
    "\n",
    "# If you expect only one match and want to get that specific row\n",
    "if len(matching_rows) == 1:\n",
    "    specific_row = matching_rows.iloc[0]\n",
    "else:\n",
    "    # If there are multiple matches, you can print or inspect them\n",
    "    specific_row = matching_rows\n",
    "\n",
    "session_id = specific_row['session_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8923db-a2fe-4c1d-9973-47b3cf5322ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to see if session id is there\n",
    "session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "680b2200-c25f-479d-beb8-2528a01c411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_test = test_df[test_df['id'] == 'd77613d1-d55b-5d4b-a7ab-27085be8cb83']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "4c533ff6-2f47-4dfb-96ce-138e1deab0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_test = merge_eda_with_stim(get_stim_timestamps(current_test), eda_streams_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a88918c2-3f3e-4e79-9bb4-cba9e3480a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = align_hrv_to_test(single_test, hrv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b61ed4-daf6-4006-bf7e-f7758e92182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
