{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69cdc0a9-9dc3-4142-9782-c8293043cff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3894ba44-23ff-49f5-b8bd-08188189e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from query.utils import Env\n",
    "env = Env.STAGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fa27f64-5553-497c-bf9f-3e38f3c722fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pison_cloud.pison.reaction.cloud.v1 import reaction_pb2, reaction_pb2_grpc\n",
    "from query.microservices import PisonGrpc\n",
    "from query.microservices import ResponseConverter\n",
    "from query.utils import Env\n",
    "\n",
    "class ReactionConverter(ResponseConverter):\n",
    "    def __call__(self, response):\n",
    "        response_dict = MessageToDict(response)\n",
    "        \n",
    "        if \"tests\" in response_dict:\n",
    "            return pd.json_normalize(response_dict['tests'])\n",
    "        else:\n",
    "            data_f = super().__call__(response)\n",
    "            return data_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85416bf8-6abb-4949-ad74-8411bf413e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'reactionTestType', 'userId', 'onsetMoments', 'comment',\n",
      "       'createdAt', 'score', 'baselineId', 'plan.id', 'plan.stimuli',\n",
      "       'plan.durationInSeconds', 'plan.userId',\n",
      "       'enrichmentData.numberOfTrials', 'enrichmentData.numberOfFalseStarts',\n",
      "       'enrichmentData.numberOfHits', 'enrichmentData.meanReactionTime',\n",
      "       'enrichmentData.stdevReactionTime', 'enrichmentData.accuracy',\n",
      "       'enrichmentData.trialResults', 'sessionId', 'isBaseline', 'isFailed',\n",
      "       'modelIdentifier', 'isDeleted', 'deletionReason'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pison_cloud.pison.reaction.cloud.v1 import reaction_pb2, reaction_pb2_grpc\n",
    "from pison_cloud.pison.common.cloud.v1 import common_pb2\n",
    "from query.microservices import PisonGrpc\n",
    "from query.utils import Env\n",
    "from google.protobuf.timestamp_pb2 import Timestamp\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "\n",
    "def get_pb_date_range(start, end):\n",
    "    return common_pb2.DateRange(start=datetime_to_timestamp(start), end=datetime_to_timestamp(end))\n",
    "\n",
    "def datetime_to_timestamp(datetime):\n",
    "    timestamp = Timestamp()\n",
    "    timestamp.FromDatetime(datetime)\n",
    "    return timestamp\n",
    "\n",
    "def timestamp_to_datetime(timestamp):\n",
    "    \"\"\"Converts Protobuf timestamp to date-aware datetime\"\"\"\n",
    "    return timestamp.ToDatetime(tzinfo=timezone.get_current_timezone())\n",
    "\n",
    "start_date = '2024-05-01 00:00:00'\n",
    "end_date = '2024-12-30 23:59:59'\n",
    "\n",
    "dt_start_date = datetime.strptime(start_date, \"%Y-%m-%d %H:%M:%S\")  # Convert string to datetime object\n",
    "dt_end_date = datetime.strptime(end_date, \"%Y-%m-%d %H:%M:%S\")  # Convert string to datetime object\n",
    "\n",
    "\n",
    "with PisonGrpc(env=env) as rpc:\n",
    "    request = common_pb2.ReadDataRequest(filter = common_pb2.ListFilterParams(date_range=get_pb_date_range(dt_start_date, dt_end_date)))\n",
    "    \n",
    "    reaction_res = rpc(\n",
    "        reaction_pb2_grpc.ReactionTestServiceStub, \"ReadReactionTests\", request, ReactionConverter()\n",
    "    )\n",
    "\n",
    "df = reaction_res['dataframe']\n",
    "# display(df)\n",
    "# ed_test = df['team_id'].notnull()\n",
    "# print(ed_test)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a82251b1-ef84-4657-bec6-dbecb4624550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pison_ready.readiness import get_score as get_readiness_score\n",
    "from pison_ready.agility import get_score as get_agility_score\n",
    "from pison_ready.focus import get_score as get_focus_score\n",
    "\n",
    "import pprint\n",
    "import time\n",
    "\n",
    "def extract_onset_times(onset_df):\n",
    "    onset_df['onset_moment'] = pd.to_datetime(onset_df['onset_moment'],format ='mixed')\n",
    "    onset_df['created_at'] = pd.to_datetime(onset_df['created_at'],format ='mixed')\n",
    "    onset_times = onset_df['onset_moment'] - onset_df['created_at']\n",
    "    onset_times = onset_times.dt.total_seconds().values\n",
    "    onset_times = sorted(onset_times)\n",
    "    return onset_times\n",
    "\n",
    "def generate_onset_df(score_df):\n",
    "    all_onset_rows = []  # To accumulate all onset rows from each test_row\n",
    "    for test_index, test_row in score_df.iterrows():\n",
    "        onset_list = test_row['onset_moments']\n",
    "        if not isinstance(onset_list, list):\n",
    "            continue  # Skip rows where onset_moments is not a valid list\n",
    "        for onset in onset_list:\n",
    "            row = {\n",
    "                'onset_moment': onset,\n",
    "                'created_at': test_row['created_at'],  # Accessing the created_at from the same dataframe\n",
    "                'uid': test_row['uid']  # Accessing the UID from the same dataframe\n",
    "            }\n",
    "            all_onset_rows.append(row)  # Append the row to the all_onset_rows list\n",
    "    return pd.DataFrame(all_onset_rows)\n",
    "\n",
    "\n",
    "def score_parity_test(uid, test, score_uid_df, plan_uid_df, config, baseline_df = None, verbose = False):\n",
    "    \n",
    "    report = {\n",
    "        'uid': uid,\n",
    "        'createdAt': score_uid_df['createdAt'].iloc[0],\n",
    "        'has_ms_precision': '.' in str(score_uid_df['createdAt'].iloc[0]),\n",
    "        'has_onset_moments': not score_uid_df['onsetMoments'].isna().any(), # score but no onset moments is weird\n",
    "        'has_plan_data': len(plan_uid_df) > 0,\n",
    "        'has_stimuli_times': (len(plan_uid_df) > 0) and (not plan_uid_df['timeInSeconds'].isna().any()),\n",
    "    }\n",
    "    \n",
    "    if pd.isnull(report['createdAt']) or (not report['has_ms_precision']) or (not report['has_onset_moments']) or not report['has_plan_data'] or not report['has_stimuli_times']:\n",
    "        return report, None\n",
    "\n",
    "    stimulus_times = plan_uid_df['timeInSeconds'].values\n",
    "    onset_times = extract_onset_times(score_uid_df)\n",
    "\n",
    "    if test == 'readiness':\n",
    "        score, info = get_readiness_score(stimulus_times, onset_times, config)\n",
    "        firmware_score = score_uid_df['reactionTimeInMilliseconds'].fillna(0).iloc[0]\n",
    "        match = abs(score - firmware_score) <= 1.0\n",
    "\n",
    "    elif test == 'agility':\n",
    "        nogo_trials = (plan_uid_df.configuration_color_blue == 0).values\n",
    "\n",
    "        # blue = 0 -> nogo\n",
    "        # blue = 1 -> go\n",
    "        # blue = 0.4 -> new nogo?\n",
    "        if np.sum(nogo_trials) == 0:\n",
    "            report['nonstandard_nogo_lighting'] = True\n",
    "            #print(f\"Skipping {uid}, no nogo trials found, possibly using a nonstandard lighting schema\")\n",
    "            return report, None\n",
    "\n",
    "        score, info = get_agility_score(stimulus_times, nogo_trials, onset_times, config)\n",
    "        firmware_score = score_uid_df['agilityScoreValue'].fillna(0).iloc[0]\n",
    "        match = (score - firmware_score) == 0\n",
    "\n",
    "    elif test == 'focus':\n",
    "        user_id = score_uid_df['userId'].iloc[0]\n",
    "        baseline_uid_df = baseline_df[baseline_df['userId'] == user_id]\n",
    "        \n",
    "        if baseline_uid_df.empty:\n",
    "            report['empty_baseline'] = True\n",
    "            #print(f'skipping {uid}, no baseline found')\n",
    "            return report, None\n",
    "        \n",
    "        # ms to seconds\n",
    "        baseline_reaction_time = baseline_uid_df['reactionTimeInMilliseconds'].iloc[0] / 1000\n",
    "        firmware_score = score_uid_df['focusScoreValue'].fillna(0).iloc[0]\n",
    "        score, info = get_focus_score(stimulus_times, onset_times, config, baseline_reaction_time)\n",
    "        match = (score - firmware_score) == 0\n",
    "\n",
    "    if not match and verbose:\n",
    "        # print('uid', uid)\n",
    "        # print('created at', score_uid_df['createdAt'].iloc[0])\n",
    "        # print('stimulus_times', stimulus_times)\n",
    "        # print('onset_times', onset_times)\n",
    "        # if 'nogo_trials' in locals():\n",
    "        #     print('nogo_trials', nogo_trials)\n",
    "        # print('score', score)\n",
    "        # print('firmware_score', firmware_score)\n",
    "        # pprint.pprint(info\n",
    "        print(info)\n",
    "\n",
    "    return report, match\n",
    "\n",
    "\n",
    "def score_parity_test_all(score_df, plan_df, test, config, baseline_df = None, verbose = False, subset = None):\n",
    "    \n",
    "    report_rows = []\n",
    "    score_df = score_df.copy()\n",
    "    plan_df = plan_df.copy()\n",
    "    \n",
    "    if subset is not None:\n",
    "        unique_uids = score_df.uid.unique()\n",
    "        rand_uids = np.random.choice(unique_uids, size = subset, replace = False)\n",
    "        score_df = score_df[score_df.uid.isin(rand_uids)]\n",
    "        \n",
    "    score_df.sort_values(by='createdAt', inplace = True)\n",
    "    print(f\"Checking Score parity on {len(score_df.uid.unique())} {test} tests...\")\n",
    "    \n",
    "    for uid, score_uid_df in score_df.groupby('uid', sort=False):\n",
    "        score_uid_df = score_df[score_df['uid'] == uid]\n",
    "        plan_uid_df = plan_df[plan_df['uid'] == uid]\n",
    "        \n",
    "        report_row, match = score_parity_test(uid, test, score_uid_df, plan_uid_df, config, baseline_df, verbose)\n",
    "        report_row['match'] = match\n",
    "        \n",
    "        report_rows.append(report_row)\n",
    "        \n",
    "    report_df = pd.DataFrame(report_rows)\n",
    "    \n",
    "    valid_report_df = report_df[~report_df['match'].isna()]\n",
    "    invalid_report_df = report_df[report_df['match'].isna()]\n",
    "    print(f\"{len(invalid_report_df)} invalid tests out of {len(report_df)} total; ({(len(invalid_report_df) / len(report_df))*100:.1f}%)\")\n",
    "    \n",
    "    score_parity = (valid_report_df['match'] == True).sum() / len(valid_report_df)\n",
    "    print(f\"{(valid_report_df['match'] == True).sum()} tests with parity out of {len(valid_report_df)} valid tests; ({score_parity*100:.1f}%)\")\n",
    "    \n",
    "    return report_df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf78578d-c4d6-4d04-9832-a4fa19e9ceb6",
   "metadata": {},
   "source": [
    "### Agility Parity LONG CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c4d0181-41e2-47d9-97fd-460756d21b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agility_config = {'detection_window': (0.07, 0.750), 'countdown': 5, 'proportion_correct_balanced': True}\n",
    "# agility_report_df, verbose_agility_df = score_parity_test_all(agility_df, agility_plan_df, 'agility', agility_config, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f58e63e5-b8e9-4946-827f-253bcfb32a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import logging\n",
    "IES_NORM_TABLE = {\n",
    "    900.0: 1,\n",
    "    885.49: 2,\n",
    "    871.21: 3,\n",
    "    857.16: 4,\n",
    "    843.34: 5,\n",
    "    829.74: 6,\n",
    "    816.36: 7,\n",
    "    803.19: 8,\n",
    "    790.24: 9,\n",
    "    777.5: 10,\n",
    "    764.96: 11,\n",
    "    752.63: 12,\n",
    "    740.49: 13,\n",
    "    728.55: 14,\n",
    "    716.8: 15,\n",
    "    705.24: 16,\n",
    "    693.87: 17,\n",
    "    682.68: 18,\n",
    "    671.67: 19,\n",
    "    660.84: 20,\n",
    "    650.18: 21,\n",
    "    639.7: 22,\n",
    "    629.38: 23,\n",
    "    619.24: 24,\n",
    "    609.25: 25,\n",
    "    599.43: 26,\n",
    "    589.76: 27,\n",
    "    580.25: 28,\n",
    "    570.89: 29,\n",
    "    561.69: 30,\n",
    "    552.63: 31,\n",
    "    543.72: 32,\n",
    "    534.95: 33,\n",
    "    526.32: 34,\n",
    "    517.84: 35,\n",
    "    509.49: 36,\n",
    "    501.27: 37,\n",
    "    493.19: 38,\n",
    "    485.23: 39,\n",
    "    477.41: 40,\n",
    "    469.71: 41,\n",
    "    462.14: 42,\n",
    "    454.68: 43,\n",
    "    447.35: 44,\n",
    "    440.14: 45,\n",
    "    433.04: 46,\n",
    "    426.06: 47,\n",
    "    419.19: 48,\n",
    "    412.43: 49,\n",
    "    405.78: 50,\n",
    "    399.23: 51,\n",
    "    392.8: 52,\n",
    "    386.46: 53,\n",
    "    380.23: 54,\n",
    "    374.1: 55,\n",
    "    368.07: 56,\n",
    "    362.13: 57,\n",
    "    356.29: 58,\n",
    "    350.55: 59,\n",
    "    344.89: 60,\n",
    "    339.33: 61,\n",
    "    333.86: 62,\n",
    "    328.48: 63,\n",
    "    323.18: 64,\n",
    "    317.97: 65,\n",
    "    312.84: 66,\n",
    "    307.8: 67,\n",
    "    302.83: 68,\n",
    "    297.95: 69,\n",
    "    293.14: 70,\n",
    "    288.42: 71,\n",
    "    283.77: 72,\n",
    "    279.19: 73,\n",
    "    274.69: 74,\n",
    "    270.26: 75,\n",
    "    265.9: 76,\n",
    "    261.61: 77,\n",
    "    257.39: 78,\n",
    "    253.24: 79,\n",
    "    249.16: 80,\n",
    "    245.14: 81,\n",
    "    241.19: 82,\n",
    "    237.3: 83,\n",
    "    233.47: 84,\n",
    "    229.71: 85,\n",
    "    226.0: 86,\n",
    "    222.36: 87,\n",
    "    218.77: 88,\n",
    "    215.25: 89,\n",
    "    211.78: 90,\n",
    "    208.36: 91,\n",
    "    205.0: 92,\n",
    "    201.69: 93,\n",
    "    198.44: 94,\n",
    "    195.24: 95,\n",
    "    192.09: 96,\n",
    "    189.0: 97,\n",
    "    185.95: 98,\n",
    "    182.95: 99,\n",
    "    180: 100,\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_ies(ies_score: float):\n",
    "    \"\"\"\n",
    "    Normalizes IES scores using predefined table.\n",
    "\n",
    "    :param ies_score: The IES score to normalize.\n",
    "    :type ies_score: float\n",
    "    :return: The normalized IES score (final agility score)\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    prev_val = 1\n",
    "    for key, value in IES_NORM_TABLE.items():\n",
    "        if ies_score < key:\n",
    "            prev_val = value\n",
    "            continue\n",
    "        return prev_val\n",
    "    return 100\n",
    "\n",
    "\n",
    "def get_trial_results(stimulus_times: List[float], onset_times: List[float], config: Dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Computes individual trial results based on stimulus and reaction times with test configuration.\n",
    "    Common logic between all three Pison Ready tests\n",
    "\n",
    "    :param stimulus_times: List of stimulus times in seconds\n",
    "    :type stimulus_times: List[float]\n",
    "    :param onset_times: List of onset times in seconds\n",
    "    :type onset_times: List[float]\n",
    "    :param config: test parameters (business logic)\n",
    "    :type config: Dict\n",
    "    :return: DataFrame that contains: miss, false start, and reaction time info for each trial\n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    trial_results = []\n",
    "    for i, led_time in enumerate(stimulus_times):\n",
    "\n",
    "        if i == 0:\n",
    "            last_led_time = -1 * config[\"detection_window\"][1] + config[\"countdown\"]\n",
    "        else:\n",
    "            last_led_time = stimulus_times[i - 1]\n",
    "\n",
    "        # This trial is from last_led_time + detection window end to led_time + detection window end\n",
    "        relevant_onset_times = [\n",
    "            x\n",
    "            for x in onset_times\n",
    "            if x > last_led_time + config[\"detection_window\"][1] and x < led_time + config[\"detection_window\"][1]\n",
    "        ]\n",
    "\n",
    "        trial_result = {\"miss\": False, \"false_start\": False, \"valid_reaction_time\": False, \"reaction_time\": None}\n",
    "\n",
    "        if len(relevant_onset_times) == 0:\n",
    "            trial_result[\"miss\"] = True\n",
    "        else:\n",
    "            reaction_time = relevant_onset_times[0] - led_time\n",
    "            trial_result[\"reaction_time\"] = reaction_time\n",
    "\n",
    "            # These three are mutually exclusive\n",
    "            if reaction_time <= config[\"detection_window\"][0]:\n",
    "                trial_result[\"false_start\"] = True\n",
    "            elif reaction_time >= config[\"detection_window\"][1]:\n",
    "                trial_result[\"miss\"] = True\n",
    "            else:\n",
    "                trial_result[\"valid_reaction_time\"] = True\n",
    "\n",
    "        logger.debug(\"trial (%s) results: %s\", i, trial_result)\n",
    "\n",
    "        trial_results.append(trial_result)\n",
    "    trial_results_df = pd.DataFrame(trial_results)\n",
    "    trial_results_df.index.name = \"trial\"\n",
    "\n",
    "    return trial_results_df\n",
    "\n",
    "def modified_get_agility_score(stimulus_times: List[float], nogo_trials: List[bool], onset_times: List[float], config: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    Calculates GNG agility score from data per configuration.\n",
    "    Specifications: https://docs.google.com/document/d/1p2WjJF6YwtrJBbaoMl0sO466jPIX2-cWUlRACBKEc2c/edit#heading=h.ebe2537u0xx5\n",
    "\n",
    "    :param stimulus_times: List of stimulus times in seconds\n",
    "    :type stimulus_times: List[float]\n",
    "    :param nogo_trials: List indicating whether each trial is a no-go trial\n",
    "    :type nogo_trials: List[bool]\n",
    "    :param onset_times: List of onset times in seconds\n",
    "    :type onset_times: List[float]\n",
    "    :param config: Configuration specifying business logic parameters\n",
    "    :type config: dict\n",
    "    :return: A tuple containing the final score and a dictionary with additional scoring information\n",
    "    :rtype: tuple\n",
    "    \"\"\"\n",
    "    # logger = logging.getLogger(__name__)\n",
    "\n",
    "    num_nogo_trials = np.sum(nogo_trials)\n",
    "    num_go_trials = len(stimulus_times) - num_nogo_trials\n",
    "\n",
    "    # pison_assert(\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    num_nogo_trials = np.sum(nogo_trials)\n",
    "    num_go_trials = len(stimulus_times) - num_nogo_trials\n",
    "    \n",
    "    if num_nogo_trials == 0 or num_go_trials == 0:\n",
    "        logger.debug(\"Number of nogo trials or go trials is zero, invalid score\")\n",
    "        # print(f\"WHY ARE THEY ALL 0 - numnogo and numgo, we got {num_nogo_trials} and {num_go_trials} so what\")\n",
    "        return None, None, None, None, None, None\n",
    "    \n",
    "    trial_results_df = get_trial_results(stimulus_times, onset_times, config)\n",
    "\n",
    "    # Add nogo_trials as a column to trial_results_df for easier access\n",
    "    trial_results_df[\"go_trial\"] = ~nogo_trials\n",
    "\n",
    "    # Calculate correct go and nogo trials and their reaction times\n",
    "    correct_go_trials = trial_results_df[trial_results_df[\"go_trial\"] & trial_results_df[\"valid_reaction_time\"]]\n",
    "    correct_nogo_trials = trial_results_df[(~trial_results_df[\"go_trial\"]) & trial_results_df[\"miss\"]]\n",
    "    correct_go = len(correct_go_trials)\n",
    "    correct_nogo = len(correct_nogo_trials)\n",
    "\n",
    "    if correct_go == 0:\n",
    "        logger.debug(\"no correct go trials, invalid score\")\n",
    "        return None, None, None, None, None, None\n",
    "        # return 0, {\"is_valid_score\": False, \"trial_results_df\": trial_results_df}\n",
    "\n",
    "    # Must come after checking for correct_go = 0\n",
    "    algo_data_mean_reaction_time = correct_go_trials[\"reaction_time\"].sum() / correct_go\n",
    "    algo_data_stdev_reaction_time = correct_go_trials[\"reaction_time\"].std()\n",
    "    \n",
    "\n",
    "    # TPR/FPR for research reasons\n",
    "    # proportion of actual positive cases that are correctly identified\n",
    "    tpr = correct_go / num_go_trials\n",
    "    # proportion of actual negative cases that are incorrectly identified as positive\n",
    "    incorrect_nogo = num_nogo_trials - correct_nogo\n",
    "    fpr = incorrect_nogo / num_nogo_trials\n",
    "    \n",
    "\n",
    "    if \"proportion_correct_balanced\" not in config:\n",
    "        config[\"proportion_correct_balanced\"] = True\n",
    "\n",
    "    if config[\"proportion_correct_balanced\"]:\n",
    "        algo_data_accuracy = ((0.7) * (correct_nogo / num_nogo_trials)) + ((0.3) * (correct_go / num_go_trials))\n",
    "    else:\n",
    "        # here for backwards compatibility for older firmware versions\n",
    "        algo_data_accuracy = (correct_go + correct_nogo) / len(stimulus_times)\n",
    "\n",
    "    if algo_data_accuracy == 0:\n",
    "        ies_score = 1\n",
    "    else:\n",
    "        ies_score = algo_data_mean_reaction_time / algo_data_accuracy\n",
    "    normalized_score = normalize_ies(ies_score * 1000)\n",
    "\n",
    "    logger.debug(\"proportion: %s\", algo_data_accuracy)\n",
    "    logger.debug(\"mean_time: %s, ies_score: %s\", algo_data_mean_reaction_time, ies_score)\n",
    "    logger.debug(\"normalized_score: %s\", normalized_score)\n",
    "    \n",
    "    algo_data_number_of_trials = len(stimulus_times)\n",
    "    algo_data_number_of_false_starts = len(trial_results_df[trial_results_df[\"false_start\"] == True])\n",
    "    algo_data_number_of_lapses = len(correct_go_trials[correct_go_trials[\"reaction_time\"] > 0.295])    \n",
    "    info = {\n",
    "        \"is_valid_score\": True,\n",
    "        \"algo_data_accuracy\": algo_data_accuracy,\n",
    "        \"algo_data_mean_reaction_time\": algo_data_mean_reaction_time,\n",
    "        \"algo_data_stdev_reaction_time\": algo_data_stdev_reaction_time,\n",
    "        # \"trial_results_df\": trial_results_df,\n",
    "        \"TPR\": tpr,\n",
    "        \"FPR\": fpr,\n",
    "    }\n",
    "\n",
    "    # return normalized_score, info\n",
    "    return algo_data_mean_reaction_time, algo_data_accuracy, algo_data_stdev_reaction_time, algo_data_number_of_trials, algo_data_number_of_false_starts, algo_data_number_of_lapses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "545459c7-82cb-4b02-bfd0-e41a1e09cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "start_date = dt.datetime(2024, 5, 1, 0, 0, 0)\n",
    "end_date = dt.datetime(2024, 12, 30, 0, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "237ed12c-d0fd-4dff-afcf-70624e5a82f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# readiness_config = {'detection_window': (0.1, 1.0), 'countdown': 0, 'retained_reaction_time_count': (1,5), 'minimum_reaction_time_count': 2}\n",
    "# readiness_report_df, verbose_readiness_df = score_parity_test_all(readiness_df, readiness_plan_df, 'readiness', readiness_config, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e83d51c-1478-45ef-9b37-479597f509e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plan_df(score_df):\n",
    "    all_stim_rows = []  # To accumulate all stim_rows from each test_row\n",
    "    for test_index, test_row in score_df.iterrows():\n",
    "        plan_list = test_row['plan.stimuli'] #had to change back to period instead of underscore since we change to underscore later TUE\n",
    "\n",
    "        if not isinstance(plan_list, list):\n",
    "            continue  # Skip rows where plan_stimuli is not a valid list\n",
    "        for stimuli in plan_list:\n",
    "            row = { #i am using .get to deal with handling missing blue for agility \n",
    "                'timeInSeconds': stimuli['timeInSeconds'],\n",
    "                'configuration_color_red': stimuli['configuration']['color']['red'],\n",
    "                'configuration_color_green': stimuli['configuration']['color']['green'],\n",
    "                'configuration_color_blue': stimuli.get('configuration', {}).get('color', {}).get('blue', None),\n",
    "                'configuration_durationInSeconds': stimuli['configuration']['durationInSeconds'],\n",
    "                'uid': test_row['uid'],\n",
    "                'id': test_row['id']\n",
    "            }\n",
    "            if row['configuration_color_blue'] is None:\n",
    "                logging.info(f\"Missing 'configuration_color_blue' for UID: {row['uid']}\")\n",
    "\n",
    "            all_stim_rows.append(row)\n",
    "    return pd.DataFrame(all_stim_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "01aa3cab-dd6d-4ca9-b12e-be3036827290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_algo_calculations(test, test_df):\n",
    "        \n",
    "    score_df = test_df\n",
    "    plan_df = generate_plan_df(test_df)\n",
    "\n",
    "    def process_ready(id, group, score_df, plan_df):\n",
    "        score_uid_df = score_df[score_df['plan.id'] == id]\n",
    "        onset_df = generate_onset_df(score_uid_df)\n",
    "        plan_uid_df = plan_df[plan_df['id'] == id]\n",
    "\n",
    "        if not onset_df.empty:\n",
    "            nogo_trials = (plan_uid_df['configuration_color_blue'] == 0).values\n",
    "            onset_times = extract_onset_times(onset_df)\n",
    "            stimulus_times = plan_uid_df['timeInSeconds'].values\n",
    "\n",
    "            algo_enrichment_data_mean_reaction_time = None\n",
    "            algo_enrichment_data_accuracy = None\n",
    "            algo_enrichment_data_stdev_reaction_time = None\n",
    "            algo_enrichment_data_number_of_trials = None\n",
    "            algo_enrichment_data_number_of_false_starts = None\n",
    "            algo_enrichment_data_number_of_lapses = None\n",
    "\n",
    "            return {\n",
    "                'uid': id,\n",
    "                'algo_enrichment_data_mean_reaction_time': algo_enrichment_data_mean_reaction_time,\n",
    "                'algo_enrichment_data_accuracy': algo_enrichment_data_accuracy,\n",
    "                'algo_enrichment_data_stdev_reaction_time': algo_enrichment_data_stdev_reaction_time,\n",
    "                'algo_enrichment_data_number_of_trials': algo_enrichment_data_number_of_trials,\n",
    "                'algo_enrichment_data_number_of_false_starts': algo_enrichment_data_number_of_false_starts,\n",
    "                'algo_enrichment_data_number_of_lapses': algo_enrichment_data_number_of_lapses\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Warning: Onset DataFrame is empty for id={id}. No data to process.\")\n",
    "            return None\n",
    "\n",
    "    def process_agility(id, group, score_df, plan_df):\n",
    "        score_uid_df = score_df[score_df['plan.id'] == id]\n",
    "        onset_df = generate_onset_df(score_uid_df)\n",
    "        plan_uid_df = plan_df[plan_df['id'] == id]\n",
    "\n",
    "        if not onset_df.empty:\n",
    "            nogo_trials = (plan_uid_df['configuration_color_blue'].isna() | (plan_uid_df['configuration_color_blue'] == 0)).values\n",
    "            onset_times = extract_onset_times(onset_df)\n",
    "            stimulus_times = plan_uid_df['timeInSeconds'].values\n",
    "\n",
    "            algo_enrichment_data_mean_reaction_time, algo_enrichment_data_accuracy, algo_enrichment_data_stdev_reaction_time, algo_enrichment_data_number_of_trials, algo_enrichment_data_number_of_false_starts, algo_enrichment_data_number_of_lapses = modified_get_agility_score(stimulus_times, nogo_trials, onset_times, agility_config)\n",
    "\n",
    "            return {\n",
    "                'uid': id,\n",
    "                'algo_enrichment_data_mean_reaction_time': algo_enrichment_data_mean_reaction_time,\n",
    "                'algo_enrichment_data_accuracy': algo_enrichment_data_accuracy,\n",
    "                'algo_enrichment_data_stdev_reaction_time': algo_enrichment_data_stdev_reaction_time,\n",
    "                'algo_enrichment_data_number_of_trials': algo_enrichment_data_number_of_trials,\n",
    "                'algo_enrichment_data_number_of_false_starts': algo_enrichment_data_number_of_false_starts,\n",
    "                'algo_enrichment_data_number_of_lapses': algo_enrichment_data_number_of_lapses\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Warning: Onset DataFrame is empty for id={id}. No data to process.\")\n",
    "            return None\n",
    "\n",
    "    def process_focus(id, group, score_df, plan_df):\n",
    "        score_uid_df = score_df[score_df['plan.id'] == id]\n",
    "        onset_df = generate_onset_df(score_uid_df)\n",
    "        plan_uid_df = plan_df[plan_df['id'] == id]\n",
    "\n",
    "        if not onset_df.empty:\n",
    "            nogo_trials = (plan_uid_df['configuration_color_blue'] == 0).values\n",
    "            onset_times = extract_onset_times(onset_df)\n",
    "            stimulus_times = plan_uid_df['timeInSeconds'].values\n",
    "\n",
    "            # Perform calculations and update these variables as needed\n",
    "            algo_enrichment_data_mean_reaction_time = None\n",
    "            algo_enrichment_data_accuracy = None\n",
    "            algo_enrichment_data_stdev_reaction_time = None\n",
    "            algo_enrichment_data_number_of_trials = None\n",
    "            algo_enrichment_data_number_of_false_starts = None\n",
    "            algo_enrichment_data_number_of_lapses = None\n",
    "\n",
    "            return {\n",
    "                'uid': id,\n",
    "                'algo_enrichment_data_mean_reaction_time': algo_enrichment_data_mean_reaction_time,\n",
    "                'algo_enrichment_data_accuracy': algo_enrichment_data_accuracy,\n",
    "                'algo_enrichment_data_stdev_reaction_time': algo_enrichment_data_stdev_reaction_time,\n",
    "                'algo_enrichment_data_number_of_trials': algo_enrichment_data_number_of_trials,\n",
    "                'algo_enrichment_data_number_of_false_starts': algo_enrichment_data_number_of_false_starts,\n",
    "                'algo_enrichment_data_number_of_lapses': algo_enrichment_data_number_of_lapses\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Warning: Onset DataFrame is empty for id={id}. No data to process.\")\n",
    "            return None\n",
    "\n",
    "    results = []\n",
    "    # result = None\n",
    "\n",
    "    for id, group in test_df.groupby('id', sort=False):\n",
    "        if test == 'READY':\n",
    "            result = process_ready(id, group, score_df, plan_df)\n",
    "        elif test == 'AGILITY':\n",
    "            result = process_agility(id, group, score_df, plan_df)\n",
    "        elif test == 'FOCUS':\n",
    "            result = process_focus(id, group, score_df, plan_df)\n",
    "        elif test is None: # this is hacky, not good\n",
    "            if process_agility(id, group, score_df, plan_df):\n",
    "                result = process_agility(id, group, score_df, plan_df)\n",
    "            elif process_ready(id, group, score_df, plan_df):\n",
    "                result = process_ready(id, group, score_df, plan_df)\n",
    "            elif process_focus(id, group, score_df, plan_df):\n",
    "                result = process_focus(id, group, score_df, plan_df)\n",
    "            else:\n",
    "                result = None\n",
    "        else:\n",
    "            print(f\"Unknown test type: {test}\")\n",
    "            continue\n",
    "\n",
    "        if result:\n",
    "            results.append(result)\n",
    "\n",
    "    algo_calculations_df = pd.DataFrame(results)\n",
    "    \n",
    "    return algo_calculations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ccadca-2bcc-4f1d-a379-dd1351fad593",
   "metadata": {},
   "source": [
    "### Creating the Final Dataframe to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bcd69b42-5521-4bf0-8533-c4ca4913f044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Onset DataFrame is empty for id=c0b30d22-f66a-530e-af64-8d0d964788c3. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=9710ecb7-5b11-5836-ad76-b64efe9b35a6. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=b4be97d5-a500-5790-87fb-e694d78ffd1c. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=5fb9b6c1-9aa3-52cf-967a-116bb0b3b601. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=d682ecea-107f-53a0-94a9-6009dbf20c31. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=733667a4-9110-5fe5-982d-206793b3bde3. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=aeb050a7-84e5-55eb-9a07-f45ad7b427e1. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=510c0966-0520-50c6-9dc7-cfbfb665fe7a. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=99a278bd-dfc5-5cf8-8581-1d565ef0f813. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=cbe94b50-d859-5419-bccf-1001bb503813. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=8b6697a3-18f4-59b7-bd4e-9dccf770728a. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=10a7988d-aeae-5cd8-ba4b-e3115300545f. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=32917d9e-2d6c-53ec-b56a-3f7ab277c654. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=c6ee93a1-7a0e-5da6-8bd8-c91271898f9c. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=c4b18a45-209b-58c4-8fd0-1148bcd18bfd. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=38a88caa-772e-5b8a-9df6-a4176191b214. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=fb150f65-8afe-5475-a567-f1e625595670. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=1be254b0-f3e1-5708-ba10-fe8b25a81dea. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=4948c752-2237-5bca-aae0-0e0558962bf0. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=f02a54e0-d29d-5a9b-b0ca-5171d80ff28c. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=533c8110-9cf1-5ec0-912e-604fc2e5c883. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=8cdedce2-5c24-5fda-ac12-bb6231c183f3. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=59cffc83-3c8e-5faf-ba3c-07c85d956e1a. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=18b9dabb-eb8b-5eef-a200-adb88c2eea71. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=910f4be5-5879-52b7-9145-2d24599afc0b. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=f7b99649-e00b-5ec9-a934-de4579dfa653. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=8f5a7820-9895-587e-a592-eec0431091ee. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=65b73382-5ed9-5d91-8f3f-e0885c526742. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=26185487-2b89-5a0e-bb3f-adf92bea40ba. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=b915080c-276d-5621-913e-c4a3dddcd7bf. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=798da2ca-fd8e-5e0a-a34a-462671b3f93a. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=cacce5f3-388b-5873-860b-cbc736c82570. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=251f3be3-bf01-5c78-bdb9-14cd3655b32e. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=091522d0-55b8-5023-8bd7-7d4e935d7392. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=aa7ef22f-1418-40d3-b29f-1842eade6237. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=54cdd72a-cc60-5f3d-a967-e7e122a8144a. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=35b2d1c9-9efd-40a5-aa39-5c986aea3cb0. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=0e96f357-b8f6-55f0-be39-d64406306734. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=85d83db2-1e7c-5c0c-83ab-8d2bda121db8. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=f9816734-78d4-5aa4-b1af-69b74afdd45f. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=ba948762-a6ae-5c98-a846-538638978b63. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=c6abe8d6-94de-4178-85ea-b354125387db. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=b2574744-2d7f-4ea1-b6cf-54ae6717cc18. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=b9fbce6e-3e00-4406-afce-59c4e8ed6317. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=099928e4-b41f-56c9-a66c-26ac58395ef4. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=fc8c59c2-61ee-5d2c-8967-cd52256815e7. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=add84473-d0ae-5ddc-ac76-8ef3cc9f96b0. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=6bdc811b-2210-5f6a-9363-d1e2759490f1. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=83c7afd1-dcf7-553b-a3ca-70ba6a86209c. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=2889e3d9-7179-5ee6-84d9-6b86b58a483f. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=89cd877d-8c2e-5798-8fca-2f8463b23d5d. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=500f5f25-323d-4475-84cd-5465330b0b44. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=ac5952d6-ca61-4472-b74b-a75559d75a11. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=5cad9a4d-b054-5638-9d75-bba9fe2215fc. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=8902f250-c12e-550d-a442-fb3390d91797. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=dd6c2157-befe-5788-8859-72ab616f117e. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=ed3dea68-836c-53c8-9fc3-260c09279b94. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=5df032c9-4b18-55b8-a858-b22b0aed7f28. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=3a211f42-a1e7-55e3-a720-6bd46b94fcb8. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=bff16520-db49-52a3-9d91-87fbfeab038f. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=3f2ea31d-bce3-55cd-8c75-1819178ef0cf. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=f420872c-80ee-5319-bcfa-5b23ea07d67f. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=9cd5df1a-678b-5432-96b7-2a8de00d4c7e. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=9987cfe4-1656-5942-b95e-33ce4fde688e. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=d2f6ff9a-cdae-5a3f-9d31-0d4a648deab5. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=281b8fc9-01b6-5b52-91fa-d015e4f47d69. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=527c5f32-c2f4-5e49-9e0f-64165060e488. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=c651b9ef-2fbe-4be8-9a44-919a182691e9. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=471c6890-3270-50ec-a761-5b825ddf41d2. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=1af77f4f-9f24-4825-aa31-b6f882b44157. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=6a0917e1-d5e0-4efd-8a7b-ea645ade336c. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=234a78ef-8f10-4b7e-a133-64ff83779b63. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=f929a8a7-d016-4787-a6d6-14105a834287. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=351c4752-1fe9-584e-9bef-fb2e0e12c654. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=cc08bc80-d336-4adc-9469-277fb7b7f330. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=68fe2bf0-8be8-58cf-b190-c88854d2f908. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=b43d19eb-ebef-5f98-8b09-3012c77f024d. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=9ad3cdf6-b1d0-5ce7-8030-5f9b597a0a6c. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=ea08c116-6ba0-5c3b-8ce7-2a495f552841. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=9a421fd3-f298-575b-bbdd-9eb50a5303c8. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=60e5f67b-1519-5c3a-826c-210c09b9a3f1. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=468dd890-48e9-56bd-bee3-3e377cc700f4. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=bb6cf711-19bb-4f4e-9098-8734221e2b15. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=c0b3da9b-088a-4e39-a1b9-9fdaca89bc5a. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=cc636e91-5789-5d51-bd08-c07e6c9c89a6. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=8f15752c-d861-538a-914c-9590fd2eccf4. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=35cacc2f-25ad-58c4-8959-6d83f9a3bceb. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=5844d271-090d-5acf-be34-a36b4e0f9bc4. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=4c8386b1-e059-5cd7-81c3-73bfd7e6951d. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=3b563c93-95b8-5df8-9a49-12d72dcd0dad. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=eb84a111-1d9d-5a24-bc3c-7a65c684240c. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=7930fd57-dede-5882-9b87-c5fd59e9a893. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=08a7df3b-abc9-5f9e-9569-aff7268a4dbd. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=011fdb71-cbfc-5c46-8dde-6b0e076bd221. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=a30f3f1a-768d-53bc-9154-62c0164d3223. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=363b5502-ab09-58eb-a782-2b0e2f9c20e1. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=f4e3b65b-e7b6-5354-a797-ad2baa17315d. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=90e8730a-8724-54b7-8315-cdbb000f28b2. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=2fe300ce-d287-5b46-b7f0-1b79eae96c64. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=9361930d-47b5-5645-88f2-972e63115455. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=5b1e45fe-c65c-5499-b901-1d2d2c6d602e. No data to process.\n",
      "Warning: Onset DataFrame is empty for id=94c672d5-8d27-5da4-8e23-853893bc01bd. No data to process.\n",
      "(3046, 31)\n"
     ]
    }
   ],
   "source": [
    "from query.microservices import get_users\n",
    "from query.microservices import get_reaction_tests, get_plan_data,get_all_metadata\n",
    "\n",
    "user_df = get_users(env)\n",
    "pison_users_df = pd.read_csv('pison_users.csv')\n",
    "env = Env.STAGING\n",
    "algo_calculations_df = []\n",
    "\n",
    "### CHANGE TO [None, 'READY', 'AGILITY', or 'FOCUS']\n",
    "test_type = None\n",
    "\n",
    "def get_specific_users(test_df, user_df):\n",
    "    # Create a list of unique, trimmed emails from the user_df\n",
    "    email_list = user_df['email'].str.split(',').explode().str.strip().unique().tolist()\n",
    "    # Filter the test_df to include only rows where the email is in the email_list\n",
    "    filtered_df = test_df[test_df['email'].isin(email_list)].reset_index(drop=True)\n",
    "    return filtered_df\n",
    "\n",
    "user_df = get_specific_users(user_df, pison_users_df)\n",
    "\n",
    "\n",
    "def create_test_df(env, user_df, algo_calculations_df, test_type, start_date, end_date):\n",
    "    tests_df = get_reaction_tests(env, start_date = start_date, end_date = end_date)\n",
    "    tests_df = pd.merge(tests_df, user_df, left_on='user_id', right_on='uid', how='inner')\n",
    "    \n",
    "    metadata_df = get_all_metadata(env)\n",
    "    metadata_df = metadata_df[['session_id','user_id','application_id','device_id','device_version']]    \n",
    "\n",
    "    if test_type is None:\n",
    "        test_df = tests_df\n",
    "        test_types = ['READY', 'AGILITY', 'FOCUS']\n",
    "        # Process and concatenate the data for each test type\n",
    "        algo_calculations_df = pd.concat(\n",
    "            [get_algo_calculations(test, tests_df[tests_df['reaction_test_type'] == test]) for test in test_types],\n",
    "            axis=0\n",
    "        ).reset_index(drop=True)    \n",
    "    else:\n",
    "        test_df = tests_df[tests_df['reaction_test_type'] == test_type]\n",
    "        algo_calculations_df = get_algo_calculations(test_type, test_df)\n",
    "        \n",
    "    \n",
    "    algo_test_df = pd.merge(test_df, algo_calculations_df, left_on='id', right_on='uid', how='left') # For agility algorithms\n",
    "    the_final_df = pd.merge(algo_test_df, metadata_df, left_on='session_id', right_on ='session_id', how='left')\n",
    "\n",
    "    the_final_df.columns = the_final_df.columns.str.replace('.', '_')\n",
    "    col_to_drop =['uid_x', 'uid_y','user_id_y','customAttributes_subscription','customAttributes_claims','is_baseline','model_identifier','deletion_reason', 'createdAt', 'onset_moments', 'plan_stimuli', 'enrichment_data_number_of_hits','enrichment_data_trial_results_trial_number', 'enrichment_data_trial_results_is_hit', 'enrichment_data_trial_results_onset_moment', 'enrichment_data_trial_results_reaction_time', 'enrichment_data_trial_results_is_false_start', 'enrichment_data_trial_results_is_lapse', 'plan_id', 'plan_user_id']\n",
    "\n",
    "    ### Flatten out some columns to make it bigquery-able\n",
    "    columns_to_convert = ['enrichment_data_mean_reaction_time','enrichment_data_accuracy','enrichment_data_trial_results_trial_number']\n",
    "    \n",
    "    for column in columns_to_convert:\n",
    "        the_final_df[column] = pd.to_numeric(the_final_df[column], errors='coerce')\n",
    "    \n",
    "    the_final_df['is_failed'] = pd.to_numeric(the_final_df['is_failed'], errors='coerce').astype('boolean')\n",
    "    the_final_df['is_failed'] = the_final_df['is_failed'].fillna(False).astype(bool)\n",
    "\n",
    "    the_final_df = the_final_df.drop(col_to_drop, axis=1)\n",
    "    the_final_df = the_final_df.rename(columns={'user_id_x':'user_id'})\n",
    "    \n",
    "    # Rename columns with 'sw_' prefix\n",
    "    the_final_df.rename(columns={\n",
    "        'enrichment_data_number_of_trials': 'sw_enrichment_data_number_of_trials',\n",
    "        'enrichment_data_number_of_false_starts': 'sw_enrichment_data_number_of_false_starts',\n",
    "        'enrichment_data_number_of_lapses': 'sw_enrichment_data_number_of_lapses',\n",
    "        'enrichment_data_mean_reaction_time': 'sw_enrichment_data_mean_reaction_time',\n",
    "        'enrichment_data_stdev_reaction_time': 'sw_enrichment_data_stdev_reaction_time',\n",
    "        'enrichment_data_accuracy': 'sw_enrichment_data_accuracy'\n",
    "    }, inplace=True)\n",
    "\n",
    "    return the_final_df\n",
    "\n",
    "big_df = create_test_df(env, user_df, algo_calculations_df, test_type, start_date, end_date)\n",
    "print(big_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3121bfd-f0c6-476d-9a10-96db9b5343e1",
   "metadata": {},
   "source": [
    "### Incorporates Team DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b820d0d9-cfe2-41ee-9b7e-fbcfb63feb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the team DataFrame and rename the 'Team' column\n",
    "team_df = pd.read_csv('pison_users.csv')\n",
    "team_df = team_df[['email', 'Team']]\n",
    "team_df.rename(columns={'Team': 'pison_team'}, inplace=True)\n",
    "team_df['email'] = team_df['email'].astype(str).str.split(',')\n",
    "\n",
    "# Explode the list into separate rows\n",
    "team_df_exploded = team_df.explode('email')\n",
    "\n",
    "# Strip any leading/trailing whitespace from email addresses\n",
    "team_df_exploded['email'] = team_df_exploded['email'].str.strip()\n",
    "\n",
    "# Mapping from email to pison_team\n",
    "email_to_team_map = team_df_exploded.set_index('email')['pison_team'].to_dict()\n",
    "\n",
    "big_df['pison_team'] = big_df['email'].map(email_to_team_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d29aa2-b181-4c0e-b085-7c5cd38f734f",
   "metadata": {},
   "source": [
    "### BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e446f10c-25cc-4a09-93ad-40fbb54fdea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 1/1 [00:00<00:00, 6689.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to BigQuery!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.oauth2 import service_account\n",
    "from pandas_gbq import to_gbq, read_gbq\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "project_id = 'core-aca65d38'\n",
    "dataset_name = 'Big_Tables'\n",
    "\n",
    "focus_table = 'Focus_Table'\n",
    "agility_table = 'Agility_Table'\n",
    "ready_table = 'Ready_Table'\n",
    "# combined_table = 'Combined_Table'\n",
    "combined_team_table = 'Testing2'\n",
    "\n",
    "\n",
    "CHOOSE_YOUR_DESTINATION_TABLE = combined_team_table    # CHOOSE YOUR TABLE DESTINATION HERE\n",
    "\n",
    "destination_table = f'{project_id}.{dataset_name}.{CHOOSE_YOUR_DESTINATION_TABLE}'\n",
    "rel_cred_path = \"/home/jupyter/local/GitHub/dataops_tools/Dogfooding Table Generator/key.json\"  # Adjust as per your directory structure\n",
    "cred_path = os.path.abspath(rel_cred_path)\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = cred_path\n",
    "credentials = service_account.Credentials.from_service_account_file(cred_path)\n",
    "\n",
    "try:\n",
    "    to_gbq(\n",
    "        team_merged_df, # CHANGE TO YOUR DF THAT YOU WANT TO PUT\n",
    "        destination_table,\n",
    "        project_id=project_id,\n",
    "        if_exists='replace',\n",
    "        credentials=credentials\n",
    "    )\n",
    "    print(\"Data successfully written to BigQuery!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing to BigQuery: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fb6b2885-12e1-4324-a89d-d8e18e9e607c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the filtered DataFrame: (3046, 32)\n"
     ]
    }
   ],
   "source": [
    "# Example date for filtering\n",
    "filter_start_date = pd.Timestamp('2024-05-01').tz_localize('UTC')\n",
    "# filter_end_date = pd.Timestamp('2024-07-01').tz_localize('UTC')\n",
    "\n",
    "# Convert the date column to datetime if it's not already and ensure it's timezone-aware\n",
    "team_merged_df['created_at'] = pd.to_datetime(team_merged_df['created_at'], utc=True)\n",
    "\n",
    "# Filter the DataFrame for dates after July 1st\n",
    "new_df = team_merged_df[team_merged_df['created_at'] > filter_start_date]\n",
    "# new_df = new_df[new_df['created_at'] < filter_end_date]\n",
    "\n",
    "# Display the shape of the filtered DataFrame\n",
    "print(\"Shape of the filtered DataFrame:\", new_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b1deb5-af0f-48cc-8c2c-13ddcbff7319",
   "metadata": {},
   "source": [
    "### Analyzing Columns and Bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7404171c-49fd-47d0-99c0-60c4f2c69e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of NaN values in the id column is 0 out of 3046:\n",
      "0.00%\n",
      "\n",
      "Number of NaN values in the reaction_test_type column is 2 out of 3046:\n",
      "0.07%\n",
      "\n",
      "Number of NaN values in the user_id column is 0 out of 3046:\n",
      "0.00%\n",
      "\n",
      "Number of NaN values in the session_id column is 2147 out of 3046:\n",
      "70.49%\n",
      "\n",
      "Number of NaN values in the team_id column is 3046 out of 3046:\n",
      "100.00%\n",
      "\n",
      "Number of NaN values in the comment column is 2995 out of 3046:\n",
      "98.33%\n",
      "\n",
      "Number of NaN values in the created_at column is 0 out of 3046:\n",
      "0.00%\n",
      "\n",
      "Number of NaN values in the is_failed column is 0 out of 3046:\n",
      "0.00%\n",
      "\n",
      "Number of NaN values in the score column is 406 out of 3046:\n",
      "13.33%\n",
      "\n",
      "Number of NaN values in the plan_duration_in_seconds column is 215 out of 3046:\n",
      "7.06%\n",
      "\n",
      "Number of NaN values in the sw_enrichment_data_number_of_trials column is 215 out of 3046:\n",
      "7.06%\n",
      "\n",
      "Number of NaN values in the sw_enrichment_data_number_of_false_starts column is 1920 out of 3046:\n",
      "63.03%\n",
      "\n",
      "Number of NaN values in the sw_enrichment_data_number_of_lapses column is 3046 out of 3046:\n",
      "100.00%\n",
      "\n",
      "Number of NaN values in the sw_enrichment_data_mean_reaction_time column is 311 out of 3046:\n",
      "10.21%\n",
      "\n",
      "Number of NaN values in the sw_enrichment_data_stdev_reaction_time column is 392 out of 3046:\n",
      "12.87%\n",
      "\n",
      "Number of NaN values in the sw_enrichment_data_accuracy column is 311 out of 3046:\n",
      "10.21%\n",
      "\n",
      "Number of NaN values in the baseline_id column is 1480 out of 3046:\n",
      "48.59%\n",
      "\n",
      "Number of NaN values in the is_deleted column is 3046 out of 3046:\n",
      "100.00%\n",
      "\n",
      "Number of NaN values in the email column is 0 out of 3046:\n",
      "0.00%\n",
      "\n",
      "Number of NaN values in the displayName column is 310 out of 3046:\n",
      "10.18%\n",
      "\n",
      "Number of NaN values in the photoUrl column is 300 out of 3046:\n",
      "9.85%\n",
      "\n",
      "Number of NaN values in the username column is 0 out of 3046:\n",
      "0.00%\n",
      "\n",
      "Number of NaN values in the algo_enrichment_data_mean_reaction_time column is 2677 out of 3046:\n",
      "87.89%\n",
      "\n",
      "Number of NaN values in the algo_enrichment_data_accuracy column is 2677 out of 3046:\n",
      "87.89%\n",
      "\n",
      "Number of NaN values in the algo_enrichment_data_stdev_reaction_time column is 2684 out of 3046:\n",
      "88.12%\n",
      "\n",
      "Number of NaN values in the algo_enrichment_data_number_of_trials column is 2677 out of 3046:\n",
      "87.89%\n",
      "\n",
      "Number of NaN values in the algo_enrichment_data_number_of_false_starts column is 2677 out of 3046:\n",
      "87.89%\n",
      "\n",
      "Number of NaN values in the algo_enrichment_data_number_of_lapses column is 2677 out of 3046:\n",
      "87.89%\n",
      "\n",
      "Number of NaN values in the application_id column is 2354 out of 3046:\n",
      "77.28%\n",
      "\n",
      "Number of NaN values in the device_id column is 2354 out of 3046:\n",
      "77.28%\n",
      "\n",
      "Number of NaN values in the device_version column is 2354 out of 3046:\n",
      "77.28%\n",
      "\n",
      "Number of NaN values in the pison_team column is 0 out of 3046:\n",
      "0.00%\n"
     ]
    }
   ],
   "source": [
    "def analyze_column(df, column_name):\n",
    "    unique_values = df[column_name].unique()\n",
    "    nan_count = df[column_name].isna().sum()\n",
    "    # print(unique_values)\n",
    "    total_count = df.shape[0]\n",
    "    nan_percentage = (nan_count / total_count) * 100\n",
    "\n",
    "    print(f\"\\nNumber of NaN values in the {column_name} column is {nan_count} out of {total_count}:\")\n",
    "    print(f\"{nan_percentage:.2f}%\")\n",
    "\n",
    "for e in team_merged_df.columns:\n",
    "    analyze_column(new_df, e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa542c35-cd6e-46d6-9ead-c4a9ddf9fe17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32650d19-42ca-4260-be51-dae03fdf1ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
