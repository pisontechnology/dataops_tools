{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cdc0a9-9dc3-4142-9782-c8293043cff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3894ba44-23ff-49f5-b8bd-08188189e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from query.utils import Env\n",
    "env = Env.STAGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa27f64-5553-497c-bf9f-3e38f3c722fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pison_cloud.pison.reaction.cloud.v1 import reaction_pb2, reaction_pb2_grpc\n",
    "from query.microservices import PisonGrpc\n",
    "from query.microservices import ResponseConverter\n",
    "from query.utils import Env\n",
    "\n",
    "class ReactionConverter(ResponseConverter):\n",
    "    def __call__(self, response):\n",
    "        response_dict = MessageToDict(response)\n",
    "        \n",
    "        if \"tests\" in response_dict:\n",
    "            return pd.json_normalize(response_dict['tests'])\n",
    "        else:\n",
    "            data_f = super().__call__(response)\n",
    "            return data_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85416bf8-6abb-4949-ad74-8411bf413e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pison_cloud.pison.reaction.cloud.v1 import reaction_pb2, reaction_pb2_grpc\n",
    "from pison_cloud.pison.common.cloud.v1 import common_pb2\n",
    "from query.microservices import PisonGrpc\n",
    "from query.utils import Env\n",
    "from google.protobuf.timestamp_pb2 import Timestamp\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "\n",
    "def get_pb_date_range(start, end):\n",
    "    return common_pb2.DateRange(start=datetime_to_timestamp(start), end=datetime_to_timestamp(end))\n",
    "\n",
    "def datetime_to_timestamp(datetime):\n",
    "    timestamp = Timestamp()\n",
    "    timestamp.FromDatetime(datetime)\n",
    "    return timestamp\n",
    "\n",
    "def timestamp_to_datetime(timestamp):\n",
    "    \"\"\"Converts Protobuf timestamp to date-aware datetime\"\"\"\n",
    "    return timestamp.ToDatetime(tzinfo=timezone.get_current_timezone())\n",
    "\n",
    "start_date = '2024-05-01 00:00:00'\n",
    "end_date = '2024-12-30 23:59:59'\n",
    "\n",
    "dt_start_date = datetime.strptime(start_date, \"%Y-%m-%d %H:%M:%S\")  # Convert string to datetime object\n",
    "dt_end_date = datetime.strptime(end_date, \"%Y-%m-%d %H:%M:%S\")  # Convert string to datetime object\n",
    "\n",
    "\n",
    "with PisonGrpc(env=env) as rpc:\n",
    "    request = common_pb2.ReadDataRequest(filter = common_pb2.ListFilterParams(date_range=get_pb_date_range(dt_start_date, dt_end_date)))\n",
    "    \n",
    "    reaction_res = rpc(\n",
    "        reaction_pb2_grpc.ReactionTestServiceStub, \"ReadReactionTests\", request, ReactionConverter()\n",
    "    )\n",
    "\n",
    "df = reaction_res['dataframe']\n",
    "print(df.columns)\n",
    "# mom_df = df\n",
    "# print(mom_df[mom_df['reactionTestType'] == 'BASEBALL_DRILL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82251b1-ef84-4657-bec6-dbecb4624550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pison_ready.readiness import get_score as get_readiness_score\n",
    "from pison_ready.agility import get_score as get_agility_score\n",
    "from pison_ready.focus import get_score as get_focus_score\n",
    "\n",
    "import pprint\n",
    "import time\n",
    "\n",
    "def extract_onset_times(onset_df):\n",
    "    onset_df['onset_moment'] = pd.to_datetime(onset_df['onset_moment'],format ='mixed')\n",
    "    onset_df['created_at'] = pd.to_datetime(onset_df['created_at'],format ='mixed')\n",
    "    onset_times = onset_df['onset_moment'] - onset_df['created_at']\n",
    "    onset_times = onset_times.dt.total_seconds().values\n",
    "    onset_times = sorted(onset_times)\n",
    "    return onset_times\n",
    "\n",
    "def generate_onset_df(score_df):\n",
    "    all_onset_rows = []  # To accumulate all onset rows from each test_row\n",
    "    for test_index, test_row in score_df.iterrows():\n",
    "        onset_list = test_row['onset_moments']\n",
    "        if not isinstance(onset_list, list):\n",
    "            continue  # Skip rows where onset_moments is not a valid list\n",
    "        for onset in onset_list:\n",
    "            row = {\n",
    "                'onset_moment': onset,\n",
    "                'created_at': test_row['created_at'],  # Accessing the created_at from the same dataframe\n",
    "                'uid': test_row['uid']  # Accessing the UID from the same dataframe\n",
    "            }\n",
    "            all_onset_rows.append(row)  # Append the row to the all_onset_rows list\n",
    "    return pd.DataFrame(all_onset_rows)\n",
    "\n",
    "def score_parity_test(uid, test, score_uid_df, plan_uid_df, config, baseline_df = None, verbose = False):\n",
    "    \n",
    "    report = {\n",
    "        'uid': uid,\n",
    "        'createdAt': score_uid_df['createdAt'].iloc[0],\n",
    "        'has_ms_precision': '.' in str(score_uid_df['createdAt'].iloc[0]),\n",
    "        'has_onset_moments': not score_uid_df['onsetMoments'].isna().any(), # score but no onset moments is weird\n",
    "        'has_plan_data': len(plan_uid_df) > 0,\n",
    "        'has_stimuli_times': (len(plan_uid_df) > 0) and (not plan_uid_df['timeInSeconds'].isna().any()),\n",
    "    }\n",
    "    \n",
    "    if pd.isnull(report['createdAt']) or (not report['has_ms_precision']) or (not report['has_onset_moments']) or not report['has_plan_data'] or not report['has_stimuli_times']:\n",
    "        return report, None\n",
    "\n",
    "    stimulus_times = plan_uid_df['timeInSeconds'].values\n",
    "    onset_times = extract_onset_times(score_uid_df)\n",
    "\n",
    "    if test == 'readiness':\n",
    "        score, info = get_readiness_score(stimulus_times, onset_times, config)\n",
    "        firmware_score = score_uid_df['reactionTimeInMilliseconds'].fillna(0).iloc[0]\n",
    "        match = abs(score - firmware_score) <= 1.0\n",
    "\n",
    "    elif test == 'agility':\n",
    "        nogo_trials = (plan_uid_df.configuration_color_blue == 0).values\n",
    "\n",
    "        # blue = 0 -> nogo\n",
    "        # blue = 1 -> go\n",
    "        # blue = 0.4 -> new nogo?\n",
    "        if np.sum(nogo_trials) == 0:\n",
    "            report['nonstandard_nogo_lighting'] = True\n",
    "            #print(f\"Skipping {uid}, no nogo trials found, possibly using a nonstandard lighting schema\")\n",
    "            return report, None\n",
    "\n",
    "        score, info = get_agility_score(stimulus_times, nogo_trials, onset_times, config)\n",
    "        firmware_score = score_uid_df['agilityScoreValue'].fillna(0).iloc[0]\n",
    "        match = (score - firmware_score) == 0\n",
    "\n",
    "    elif test == 'focus':\n",
    "        user_id = score_uid_df['userId'].iloc[0]\n",
    "        baseline_uid_df = baseline_df[baseline_df['userId'] == user_id]\n",
    "        \n",
    "        if baseline_uid_df.empty:\n",
    "            report['empty_baseline'] = True\n",
    "            #print(f'skipping {uid}, no baseline found')\n",
    "            return report, None\n",
    "        \n",
    "        # ms to seconds\n",
    "        baseline_reaction_time = baseline_uid_df['reactionTimeInMilliseconds'].iloc[0] / 1000\n",
    "        firmware_score = score_uid_df['focusScoreValue'].fillna(0).iloc[0]\n",
    "        score, info = get_focus_score(stimulus_times, onset_times, config, baseline_reaction_time)\n",
    "        match = (score - firmware_score) == 0\n",
    "\n",
    "    if not match and verbose:\n",
    "        # print('uid', uid)\n",
    "        # print('created at', score_uid_df['createdAt'].iloc[0])\n",
    "        # print('stimulus_times', stimulus_times)\n",
    "        # print('onset_times', onset_times)\n",
    "        # if 'nogo_trials' in locals():\n",
    "        #     print('nogo_trials', nogo_trials)\n",
    "        # print('score', score)\n",
    "        # print('firmware_score', firmware_score)\n",
    "        # pprint.pprint(info\n",
    "        print(info)\n",
    "\n",
    "    return report, match\n",
    "\n",
    "\n",
    "def score_parity_test_all(score_df, plan_df, test, config, baseline_df = None, verbose = False, subset = None):\n",
    "    \n",
    "    report_rows = []\n",
    "    score_df = score_df.copy()\n",
    "    plan_df = plan_df.copy()\n",
    "    \n",
    "    if subset is not None:\n",
    "        unique_uids = score_df.uid.unique()\n",
    "        rand_uids = np.random.choice(unique_uids, size = subset, replace = False)\n",
    "        score_df = score_df[score_df.uid.isin(rand_uids)]\n",
    "        \n",
    "    score_df.sort_values(by='createdAt', inplace = True)\n",
    "    print(f\"Checking Score parity on {len(score_df.uid.unique())} {test} tests...\")\n",
    "    \n",
    "    for uid, score_uid_df in score_df.groupby('uid', sort=False):\n",
    "        score_uid_df = score_df[score_df['uid'] == uid]\n",
    "        plan_uid_df = plan_df[plan_df['uid'] == uid]\n",
    "        \n",
    "        report_row, match = score_parity_test(uid, test, score_uid_df, plan_uid_df, config, baseline_df, verbose)\n",
    "        report_row['match'] = match\n",
    "        \n",
    "        report_rows.append(report_row)\n",
    "        \n",
    "    report_df = pd.DataFrame(report_rows)\n",
    "    \n",
    "    valid_report_df = report_df[~report_df['match'].isna()]\n",
    "    invalid_report_df = report_df[report_df['match'].isna()]\n",
    "    print(f\"{len(invalid_report_df)} invalid tests out of {len(report_df)} total; ({(len(invalid_report_df) / len(report_df))*100:.1f}%)\")\n",
    "    \n",
    "    score_parity = (valid_report_df['match'] == True).sum() / len(valid_report_df)\n",
    "    print(f\"{(valid_report_df['match'] == True).sum()} tests with parity out of {len(valid_report_df)} valid tests; ({score_parity*100:.1f}%)\")\n",
    "    \n",
    "    return report_df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf78578d-c4d6-4d04-9832-a4fa19e9ceb6",
   "metadata": {},
   "source": [
    "### Agility Parity LONG CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4d0181-41e2-47d9-97fd-460756d21b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agility_config = {'detection_window': (0.07, 0.750), 'countdown': 5, 'proportion_correct_balanced': True}\n",
    "# agility_report_df, verbose_agility_df = score_parity_test_all(agility_df, agility_plan_df, 'agility', agility_config, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58e63e5-b8e9-4946-827f-253bcfb32a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import logging\n",
    "IES_NORM_TABLE = {\n",
    "    900.0: 1,\n",
    "    885.49: 2,\n",
    "    871.21: 3,\n",
    "    857.16: 4,\n",
    "    843.34: 5,\n",
    "    829.74: 6,\n",
    "    816.36: 7,\n",
    "    803.19: 8,\n",
    "    790.24: 9,\n",
    "    777.5: 10,\n",
    "    764.96: 11,\n",
    "    752.63: 12,\n",
    "    740.49: 13,\n",
    "    728.55: 14,\n",
    "    716.8: 15,\n",
    "    705.24: 16,\n",
    "    693.87: 17,\n",
    "    682.68: 18,\n",
    "    671.67: 19,\n",
    "    660.84: 20,\n",
    "    650.18: 21,\n",
    "    639.7: 22,\n",
    "    629.38: 23,\n",
    "    619.24: 24,\n",
    "    609.25: 25,\n",
    "    599.43: 26,\n",
    "    589.76: 27,\n",
    "    580.25: 28,\n",
    "    570.89: 29,\n",
    "    561.69: 30,\n",
    "    552.63: 31,\n",
    "    543.72: 32,\n",
    "    534.95: 33,\n",
    "    526.32: 34,\n",
    "    517.84: 35,\n",
    "    509.49: 36,\n",
    "    501.27: 37,\n",
    "    493.19: 38,\n",
    "    485.23: 39,\n",
    "    477.41: 40,\n",
    "    469.71: 41,\n",
    "    462.14: 42,\n",
    "    454.68: 43,\n",
    "    447.35: 44,\n",
    "    440.14: 45,\n",
    "    433.04: 46,\n",
    "    426.06: 47,\n",
    "    419.19: 48,\n",
    "    412.43: 49,\n",
    "    405.78: 50,\n",
    "    399.23: 51,\n",
    "    392.8: 52,\n",
    "    386.46: 53,\n",
    "    380.23: 54,\n",
    "    374.1: 55,\n",
    "    368.07: 56,\n",
    "    362.13: 57,\n",
    "    356.29: 58,\n",
    "    350.55: 59,\n",
    "    344.89: 60,\n",
    "    339.33: 61,\n",
    "    333.86: 62,\n",
    "    328.48: 63,\n",
    "    323.18: 64,\n",
    "    317.97: 65,\n",
    "    312.84: 66,\n",
    "    307.8: 67,\n",
    "    302.83: 68,\n",
    "    297.95: 69,\n",
    "    293.14: 70,\n",
    "    288.42: 71,\n",
    "    283.77: 72,\n",
    "    279.19: 73,\n",
    "    274.69: 74,\n",
    "    270.26: 75,\n",
    "    265.9: 76,\n",
    "    261.61: 77,\n",
    "    257.39: 78,\n",
    "    253.24: 79,\n",
    "    249.16: 80,\n",
    "    245.14: 81,\n",
    "    241.19: 82,\n",
    "    237.3: 83,\n",
    "    233.47: 84,\n",
    "    229.71: 85,\n",
    "    226.0: 86,\n",
    "    222.36: 87,\n",
    "    218.77: 88,\n",
    "    215.25: 89,\n",
    "    211.78: 90,\n",
    "    208.36: 91,\n",
    "    205.0: 92,\n",
    "    201.69: 93,\n",
    "    198.44: 94,\n",
    "    195.24: 95,\n",
    "    192.09: 96,\n",
    "    189.0: 97,\n",
    "    185.95: 98,\n",
    "    182.95: 99,\n",
    "    180: 100,\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_ies(ies_score: float):\n",
    "    \"\"\"\n",
    "    Normalizes IES scores using predefined table.\n",
    "\n",
    "    :param ies_score: The IES score to normalize.\n",
    "    :type ies_score: float\n",
    "    :return: The normalized IES score (final agility score)\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    prev_val = 1\n",
    "    for key, value in IES_NORM_TABLE.items():\n",
    "        if ies_score < key:\n",
    "            prev_val = value\n",
    "            continue\n",
    "        return prev_val\n",
    "    return 100\n",
    "\n",
    "\n",
    "def get_trial_results(stimulus_times: List[float], onset_times: List[float], config: Dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Computes individual trial results based on stimulus and reaction times with test configuration.\n",
    "    Common logic between all three Pison Ready tests\n",
    "\n",
    "    :param stimulus_times: List of stimulus times in seconds\n",
    "    :type stimulus_times: List[float]\n",
    "    :param onset_times: List of onset times in seconds\n",
    "    :type onset_times: List[float]\n",
    "    :param config: test parameters (business logic)\n",
    "    :type config: Dict\n",
    "    :return: DataFrame that contains: miss, false start, and reaction time info for each trial\n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    trial_results = []\n",
    "    for i, led_time in enumerate(stimulus_times):\n",
    "\n",
    "        if i == 0:\n",
    "            last_led_time = -1 * config[\"detection_window\"][1] + config[\"countdown\"]\n",
    "        else:\n",
    "            last_led_time = stimulus_times[i - 1]\n",
    "\n",
    "        # This trial is from last_led_time + detection window end to led_time + detection window end\n",
    "        relevant_onset_times = [\n",
    "            x\n",
    "            for x in onset_times\n",
    "            if x > last_led_time + config[\"detection_window\"][1] and x < led_time + config[\"detection_window\"][1]\n",
    "        ]\n",
    "\n",
    "        trial_result = {\"miss\": False, \"false_start\": False, \"valid_reaction_time\": False, \"reaction_time\": None}\n",
    "\n",
    "        if len(relevant_onset_times) == 0:\n",
    "            trial_result[\"miss\"] = True\n",
    "        else:\n",
    "            reaction_time = relevant_onset_times[0] - led_time\n",
    "            trial_result[\"reaction_time\"] = reaction_time\n",
    "\n",
    "            # These three are mutually exclusive\n",
    "            if reaction_time <= config[\"detection_window\"][0]:\n",
    "                trial_result[\"false_start\"] = True\n",
    "            elif reaction_time >= config[\"detection_window\"][1]:\n",
    "                trial_result[\"miss\"] = True\n",
    "            else:\n",
    "                trial_result[\"valid_reaction_time\"] = True\n",
    "\n",
    "        logger.debug(\"trial (%s) results: %s\", i, trial_result)\n",
    "\n",
    "        trial_results.append(trial_result)\n",
    "    trial_results_df = pd.DataFrame(trial_results)\n",
    "    trial_results_df.index.name = \"trial\"\n",
    "\n",
    "    return trial_results_df\n",
    "\n",
    "def modified_get_agility_score(stimulus_times: List[float], nogo_trials: List[bool], onset_times: List[float], config: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    Calculates GNG agility score from data per configuration.\n",
    "    Specifications: https://docs.google.com/document/d/1p2WjJF6YwtrJBbaoMl0sO466jPIX2-cWUlRACBKEc2c/edit#heading=h.ebe2537u0xx5\n",
    "\n",
    "    :param stimulus_times: List of stimulus times in seconds\n",
    "    :type stimulus_times: List[float]\n",
    "    :param nogo_trials: List indicating whether each trial is a no-go trial\n",
    "    :type nogo_trials: List[bool]\n",
    "    :param onset_times: List of onset times in seconds\n",
    "    :type onset_times: List[float]\n",
    "    :param config: Configuration specifying business logic parameters\n",
    "    :type config: dict\n",
    "    :return: A tuple containing the final score and a dictionary with additional scoring information\n",
    "    :rtype: tuple\n",
    "    \"\"\"\n",
    "    # logger = logging.getLogger(__name__)\n",
    "\n",
    "    num_nogo_trials = np.sum(nogo_trials)\n",
    "    num_go_trials = len(stimulus_times) - num_nogo_trials\n",
    "\n",
    "    # pison_assert(\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    num_nogo_trials = np.sum(nogo_trials)\n",
    "    num_go_trials = len(stimulus_times) - num_nogo_trials\n",
    "    \n",
    "    if num_nogo_trials == 0 or num_go_trials == 0:\n",
    "        logger.debug(\"Number of nogo trials or go trials is zero, invalid score\")\n",
    "        return None, None, None, None, None, None\n",
    "    \n",
    "    trial_results_df = get_trial_results(stimulus_times, onset_times, config)\n",
    "\n",
    "    # Add nogo_trials as a column to trial_results_df for easier access\n",
    "    trial_results_df[\"go_trial\"] = ~nogo_trials\n",
    "\n",
    "    # Calculate correct go and nogo trials and their reaction times\n",
    "    correct_go_trials = trial_results_df[trial_results_df[\"go_trial\"] & trial_results_df[\"valid_reaction_time\"]]\n",
    "    correct_nogo_trials = trial_results_df[(~trial_results_df[\"go_trial\"]) & trial_results_df[\"miss\"]]\n",
    "    correct_go = len(correct_go_trials)\n",
    "    correct_nogo = len(correct_nogo_trials)\n",
    "\n",
    "    if correct_go == 0:\n",
    "        logger.debug(\"no correct go trials, invalid score\")\n",
    "        return None, None, None, None, None, None\n",
    "        # return 0, {\"is_valid_score\": False, \"trial_results_df\": trial_results_df}\n",
    "\n",
    "    # Must come after checking for correct_go = 0\n",
    "    algo_data_mean_reaction_time = correct_go_trials[\"reaction_time\"].sum() / correct_go\n",
    "    algo_data_stdev_reaction_time = correct_go_trials[\"reaction_time\"].std()\n",
    "    \n",
    "\n",
    "    # TPR/FPR for research reasons\n",
    "    # proportion of actual positive cases that are correctly identified\n",
    "    tpr = correct_go / num_go_trials\n",
    "    # proportion of actual negative cases that are incorrectly identified as positive\n",
    "    incorrect_nogo = num_nogo_trials - correct_nogo\n",
    "    fpr = incorrect_nogo / num_nogo_trials\n",
    "    \n",
    "\n",
    "    if \"proportion_correct_balanced\" not in config:\n",
    "        config[\"proportion_correct_balanced\"] = True\n",
    "\n",
    "    if config[\"proportion_correct_balanced\"]:\n",
    "        algo_data_accuracy = ((0.7) * (correct_nogo / num_nogo_trials)) + ((0.3) * (correct_go / num_go_trials))\n",
    "    else:\n",
    "        # here for backwards compatibility for older firmware versions\n",
    "        algo_data_accuracy = (correct_go + correct_nogo) / len(stimulus_times)\n",
    "\n",
    "    if algo_data_accuracy == 0:\n",
    "        ies_score = 1\n",
    "    else:\n",
    "        ies_score = algo_data_mean_reaction_time / algo_data_accuracy\n",
    "    normalized_score = normalize_ies(ies_score * 1000)\n",
    "\n",
    "    logger.debug(\"proportion: %s\", algo_data_accuracy)\n",
    "    logger.debug(\"mean_time: %s, ies_score: %s\", algo_data_mean_reaction_time, ies_score)\n",
    "    logger.debug(\"normalized_score: %s\", normalized_score)\n",
    "    \n",
    "    algo_data_number_of_trials = len(stimulus_times)\n",
    "    algo_data_number_of_false_starts = len(trial_results_df[trial_results_df[\"false_start\"] == True])\n",
    "    algo_data_number_of_lapses = len(correct_go_trials[correct_go_trials[\"reaction_time\"] > 0.295])    \n",
    "    info = {\n",
    "        \"is_valid_score\": True,\n",
    "        \"algo_data_accuracy\": algo_data_accuracy,\n",
    "        \"algo_data_mean_reaction_time\": algo_data_mean_reaction_time,\n",
    "        \"algo_data_stdev_reaction_time\": algo_data_stdev_reaction_time,\n",
    "        # \"trial_results_df\": trial_results_df,\n",
    "        \"TPR\": tpr,\n",
    "        \"FPR\": fpr,\n",
    "    }\n",
    "\n",
    "    # return normalized_score, info\n",
    "    return algo_data_mean_reaction_time, algo_data_accuracy, algo_data_stdev_reaction_time, algo_data_number_of_trials, algo_data_number_of_false_starts, algo_data_number_of_lapses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545459c7-82cb-4b02-bfd0-e41a1e09cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "start_date = dt.datetime(2024, 5, 1, 0, 0, 0)\n",
    "end_date = dt.datetime(2024, 12, 30, 0, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237ed12c-d0fd-4dff-afcf-70624e5a82f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "readiness_config = {'detection_window': (0.1, 1.0), 'countdown': 0, 'retained_reaction_time_count': (1,5), 'minimum_reaction_time_count': 2}\n",
    "focus_config= {'detection_window': (0.08, 1.0), 'countdown': 5, 'retained_reaction_time_count': (0,90), 'minimum_reaction_time_count': 2, 'lapse_multiplier':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e83d51c-1478-45ef-9b37-479597f509e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plan_df(score_df):\n",
    "    all_stim_rows = []  # To accumulate all stim_rows from each test_row\n",
    "    for test_index, test_row in score_df.iterrows():\n",
    "        plan_list = test_row['plan.stimuli'] #had to change back to period instead of underscore since we change to underscore later TUE\n",
    "\n",
    "        if not isinstance(plan_list, list):\n",
    "            continue  # Skip rows where plan_stimuli is not a valid list\n",
    "        for stimuli in plan_list:\n",
    "            row = { #i am using .get to deal with handling missing blue for agility \n",
    "                'timeInSeconds': stimuli['timeInSeconds'],\n",
    "                'configuration_color_red': stimuli['configuration']['color']['red'],\n",
    "                'configuration_color_green': stimuli['configuration']['color']['green'],\n",
    "                'configuration_color_blue': stimuli.get('configuration', {}).get('color', {}).get('blue', None),\n",
    "                'configuration_durationInSeconds': stimuli['configuration']['durationInSeconds'],\n",
    "                'uid': test_row['uid'],\n",
    "                'id': test_row['id']\n",
    "            }\n",
    "            if row['configuration_color_blue'] is None:\n",
    "                logging.info(f\"Missing 'configuration_color_blue' for UID: {row['uid']}\")\n",
    "\n",
    "            all_stim_rows.append(row)\n",
    "    return pd.DataFrame(all_stim_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aa3cab-dd6d-4ca9-b12e-be3036827290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_algo_calculations(test, test_df, users_df):\n",
    "    score_df = test_df\n",
    "        \n",
    "    #for agility stuff temporarilty\n",
    "    plan_df = generate_plan_df(test_df)\n",
    "    \n",
    "    def process_ready(score_df):\n",
    "        pvt_df = format_df(users_df, readiness_config, start_date, end_date, test_type='readiness')\n",
    "        PVT_results = score_pvt_results(pvt_df)\n",
    "        \n",
    "        return {\n",
    "            'uid': PVT_results['test_uid'],\n",
    "            'algo_enrichment_data_mean_reaction_time': PVT_results['mean_reaction_time'],\n",
    "            'algo_enrichment_data_accuracy': None,  # Replace with the appropriate value if needed\n",
    "            'algo_enrichment_data_stdev_reaction_time': PVT_results['standard_deviation'],\n",
    "            'algo_enrichment_data_number_of_trials': PVT_results['total_trials'],\n",
    "            'algo_enrichment_data_number_of_false_starts': PVT_results['total_fs'],\n",
    "            'algo_enrichment_data_number_of_lapses': PVT_results['total_lapses'],\n",
    "        }\n",
    "\n",
    "\n",
    "    def process_agility(id, group, score_df, plan_df):\n",
    "        # agility_df = format_df(users_df, agility_config, start_date, end_date, test_type='agility')\n",
    "\n",
    "        score_uid_df = score_df[score_df['plan.id'] == id]\n",
    "        onset_df = generate_onset_df(score_uid_df)\n",
    "        plan_uid_df = plan_df[plan_df['id'] == id]\n",
    "\n",
    "        if not onset_df.empty:\n",
    "            nogo_trials = (plan_uid_df['configuration_color_blue'].isna() | (plan_uid_df['configuration_color_blue'] == 0)).values\n",
    "            onset_times = extract_onset_times(onset_df)\n",
    "            stimulus_times = plan_uid_df['timeInSeconds'].values\n",
    "\n",
    "            algo_enrichment_data_mean_reaction_time, algo_enrichment_data_accuracy, algo_enrichment_data_stdev_reaction_time, algo_enrichment_data_number_of_trials, algo_enrichment_data_number_of_false_starts, algo_enrichment_data_number_of_lapses = modified_get_agility_score(stimulus_times, nogo_trials, onset_times, agility_config)\n",
    "\n",
    "            return {\n",
    "                'uid': id,\n",
    "                'algo_enrichment_data_mean_reaction_time': algo_enrichment_data_mean_reaction_time,\n",
    "                'algo_enrichment_data_accuracy': algo_enrichment_data_accuracy,\n",
    "                'algo_enrichment_data_stdev_reaction_time': algo_enrichment_data_stdev_reaction_time,\n",
    "                'algo_enrichment_data_number_of_trials': algo_enrichment_data_number_of_trials,\n",
    "                'algo_enrichment_data_number_of_false_starts': algo_enrichment_data_number_of_false_starts,\n",
    "                'algo_enrichment_data_number_of_lapses': algo_enrichment_data_number_of_lapses\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Warning: Onset DataFrame is empty for id={id}. No data to process.\")\n",
    "            return None\n",
    "\n",
    "    def process_focus(score_df):\n",
    "        \n",
    "        pvt_df = format_df(users_df, focus_config, start_date, end_date, test_type='focus')\n",
    "        PVT_results = score_pvt_results(pvt_df)\n",
    "            \n",
    "        return {\n",
    "            'uid': PVT_results['test_uid'],\n",
    "            'algo_enrichment_data_mean_reaction_time': PVT_results['mean_reaction_time'],\n",
    "            'algo_enrichment_data_accuracy': None,  # Replace with the appropriate value if needed\n",
    "            'algo_enrichment_data_stdev_reaction_time': PVT_results['standard_deviation'],\n",
    "            'algo_enrichment_data_number_of_trials': PVT_results['total_trials'],\n",
    "            'algo_enrichment_data_number_of_false_starts': PVT_results['total_fs'],\n",
    "            'algo_enrichment_data_number_of_lapses': PVT_results['total_lapses'],\n",
    "            #make sure to exclude them for the big table\n",
    "            # 'algo_enrichment_data_fastest_10': PVT_results['fastest_10'],\n",
    "            # 'algo_enrichment_data_slowest_10': PVT_results['slowest_10']\n",
    "        }\n",
    "\n",
    "    \n",
    "    results = []\n",
    "    result = None\n",
    "\n",
    "    if test == 'READY':\n",
    "        result = process_ready(score_df)\n",
    "        # result = None\n",
    "    elif test == 'AGILITY':\n",
    "        for id, group in test_df.groupby('id', sort=False):\n",
    "            if test == 'AGILITY':\n",
    "                result = process_agility(id, group, score_df, plan_df)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "                \n",
    "        algo_calculations_df = pd.DataFrame(results)\n",
    "        return algo_calculations_df\n",
    "    \n",
    "    elif test == 'FOCUS':\n",
    "        result = process_focus(score_df)\n",
    "    else:\n",
    "        print(f\"Unknown test type: {test}\")\n",
    "\n",
    "    algo_calculations_df = pd.DataFrame(result)\n",
    "\n",
    "    return algo_calculations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ccadca-2bcc-4f1d-a379-dd1351fad593",
   "metadata": {},
   "source": [
    "### Creating the Final Dataframe to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd69b42-5521-4bf0-8533-c4ca4913f044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from query.microservices import get_users\n",
    "from query.microservices import get_reaction_tests, get_plan_data,get_all_metadata\n",
    "from module_101_217 import *\n",
    "\n",
    "\n",
    "\n",
    "user_df = get_users(env)\n",
    "pison_users_df = pd.read_csv('pison_users.csv')\n",
    "env = Env.STAGING\n",
    "algo_calculations_df = []\n",
    "\n",
    "### CHANGE TO [None, 'READY', 'AGILITY', or 'FOCUS']\n",
    "test_type = None\n",
    "\n",
    "def get_specific_users(test_df, user_df):\n",
    "    # Create a list of unique, trimmed emails from the user_df\n",
    "    email_list = user_df['email'].str.split(',').explode().str.strip().unique().tolist()\n",
    "    # Filter the test_df to include only rows where the email is in the email_list\n",
    "    filtered_df = test_df[test_df['email'].isin(email_list)].reset_index(drop=True)\n",
    "    return filtered_df\n",
    "\n",
    "users_df = get_specific_users(user_df, pison_users_df)\n",
    "\n",
    "\n",
    "def create_test_df(env, user_df, algo_calculations_df, test_type, start_date, end_date):\n",
    "    tests_df = get_reaction_tests(env, start_date = start_date, end_date = end_date)\n",
    "    tests_df = pd.merge(tests_df, user_df, left_on='user_id', right_on='uid', how='inner')\n",
    "    \n",
    "    metadata_df = get_all_metadata(env)\n",
    "    metadata_df = metadata_df[['session_id','user_id','application_id','device_id','device_version']]    \n",
    "\n",
    "    if test_type is None:\n",
    "        test_df = tests_df\n",
    "        test_types = ['AGILITY', 'FOCUS', 'READY']\n",
    "        algo_calculations_df = pd.concat(\n",
    "            [get_algo_calculations(test, tests_df[tests_df['reaction_test_type'] == test], users_df) for test in test_types],\n",
    "            axis=0\n",
    "        ).reset_index(drop=True)    \n",
    "    else:\n",
    "        test_df = tests_df[tests_df['reaction_test_type'] == test_type]\n",
    "        algo_calculations_df = get_algo_calculations(test_type, test_df, users_df)\n",
    "        \n",
    "    \n",
    "    # print(f\"Printing Algorithm Columns: {algo_calculations_df.columns}\")\n",
    "    algo_test_df = pd.merge(test_df, algo_calculations_df, left_on='id', right_on='uid', how='left') # For agility algorithms\n",
    "    the_final_df = pd.merge(algo_test_df, metadata_df, left_on='session_id', right_on ='session_id', how='left')\n",
    "\n",
    "    the_final_df.columns = the_final_df.columns.str.replace('.', '_')\n",
    "    col_to_drop =['uid_x', 'uid_y','user_id_y','customAttributes_subscription','customAttributes_claims','is_baseline','model_identifier','deletion_reason', 'createdAt', 'onset_moments', 'plan_stimuli', 'enrichment_data_number_of_hits','enrichment_data_trial_results_trial_number', 'enrichment_data_trial_results_is_hit', 'enrichment_data_trial_results_onset_moment', 'enrichment_data_trial_results_reaction_time', 'enrichment_data_trial_results_is_false_start', 'enrichment_data_trial_results_is_lapse', 'plan_id', 'plan_user_id']\n",
    "\n",
    "    ### Flatten out some columns to make it bigquery-able\n",
    "    columns_to_convert = ['enrichment_data_mean_reaction_time','enrichment_data_accuracy','enrichment_data_trial_results_trial_number', 'algo_enrichment_data_accuracy']\n",
    "    \n",
    "    for column in columns_to_convert:\n",
    "        the_final_df[column] = pd.to_numeric(the_final_df[column], errors='coerce')\n",
    "    \n",
    "    the_final_df['is_failed'] = pd.to_numeric(the_final_df['is_failed'], errors='coerce').astype('boolean')\n",
    "    the_final_df['is_failed'] = the_final_df['is_failed'].fillna(False).astype(bool)\n",
    "\n",
    "    the_final_df = the_final_df.drop(col_to_drop, axis=1)\n",
    "    the_final_df = the_final_df.rename(columns={'user_id_x':'user_id'})\n",
    "    \n",
    "    # Rename columns with 'sw_' prefix\n",
    "    the_final_df.rename(columns={\n",
    "        'enrichment_data_number_of_trials': 'sw_enrichment_data_number_of_trials',\n",
    "        'enrichment_data_number_of_false_starts': 'sw_enrichment_data_number_of_false_starts',\n",
    "        'enrichment_data_number_of_lapses': 'sw_enrichment_data_number_of_lapses',\n",
    "        'enrichment_data_mean_reaction_time': 'sw_enrichment_data_mean_reaction_time',\n",
    "        'enrichment_data_stdev_reaction_time': 'sw_enrichment_data_stdev_reaction_time',\n",
    "        'enrichment_data_accuracy': 'sw_enrichment_data_accuracy'\n",
    "    }, inplace=True)\n",
    "\n",
    "    return the_final_df\n",
    "\n",
    "big_df = create_test_df(env, users_df, algo_calculations_df, test_type, start_date, end_date)\n",
    "print(big_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3121bfd-f0c6-476d-9a10-96db9b5343e1",
   "metadata": {},
   "source": [
    "### Incorporates Team DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b820d0d9-cfe2-41ee-9b7e-fbcfb63feb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the team DataFrame and rename the 'Team' column\n",
    "team_df = pd.read_csv('pison_users.csv')\n",
    "team_df = team_df[['email', 'Team']]\n",
    "team_df.rename(columns={'Team': 'pison_team'}, inplace=True)\n",
    "team_df['email'] = team_df['email'].astype(str).str.split(',')\n",
    "\n",
    "# Explode the list into separate rows\n",
    "team_df_exploded = team_df.explode('email')\n",
    "\n",
    "# Strip any leading/trailing whitespace from email addresses\n",
    "team_df_exploded['email'] = team_df_exploded['email'].str.strip()\n",
    "\n",
    "# Mapping from email to pison_team\n",
    "email_to_team_map = team_df_exploded.set_index('email')['pison_team'].to_dict()\n",
    "\n",
    "big_df['pison_team'] = big_df['email'].map(email_to_team_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d29aa2-b181-4c0e-b085-7c5cd38f734f",
   "metadata": {},
   "source": [
    "### BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e446f10c-25cc-4a09-93ad-40fbb54fdea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.oauth2 import service_account\n",
    "from pandas_gbq import to_gbq, read_gbq\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "project_id = 'core-aca65d38'\n",
    "dataset_name = 'Big_Tables'\n",
    "\n",
    "focus_table = 'Focus_Table'\n",
    "agility_table = 'Agility_Table'\n",
    "ready_table = 'Ready_Table'\n",
    "super_table = 'Super_Table'\n",
    "\n",
    "\n",
    "\n",
    "CHOOSE_YOUR_DESTINATION_TABLE = super_table   # CHOOSE YOUR TABLE DESTINATION HERE\n",
    "\n",
    "destination_table = f'{project_id}.{dataset_name}.{CHOOSE_YOUR_DESTINATION_TABLE}'\n",
    "rel_cred_path = \"/home/jupyter/local/Final Github/dataops_tools/Dogfooding Table Generator/key.json\"  # Adjust as per your directory structure\n",
    "cred_path = os.path.abspath(rel_cred_path)\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = cred_path\n",
    "credentials = service_account.Credentials.from_service_account_file(cred_path)\n",
    "\n",
    "try:\n",
    "    to_gbq(\n",
    "        big_df, # CHANGE TO YOUR DF THAT YOU WANT TO PUT\n",
    "        destination_table,\n",
    "        project_id=project_id,\n",
    "        if_exists='replace',\n",
    "        credentials=credentials\n",
    "    )\n",
    "    print(\"Data successfully written to BigQuery!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing to BigQuery: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6b2885-12e1-4324-a89d-d8e18e9e607c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example date for filtering\n",
    "new_df = big_df\n",
    "filter_start_date = pd.Timestamp('2024-05-01').tz_localize('UTC')\n",
    "# filter_end_date = pd.Timestamp('2024-07-01').tz_localize('UTC')\n",
    "\n",
    "# Convert the date column to datetime if it's not already and ensure it's timezone-aware\n",
    "new_df['created_at'] = pd.to_datetime(new_df['created_at'], utc=True)\n",
    "\n",
    "# Filter the DataFrame for dates after July 1st\n",
    "new_df = new_df[new_df['created_at'] > filter_start_date]\n",
    "# new_df = new_df[new_df['created_at'] < filter_end_date]\n",
    "\n",
    "# Display the shape of the filtered DataFrame\n",
    "print(\"Shape of the filtered DataFrame:\", new_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b1deb5-af0f-48cc-8c2c-13ddcbff7319",
   "metadata": {},
   "source": [
    "### Analyzing Columns and Bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7404171c-49fd-47d0-99c0-60c4f2c69e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_column(df, column_name):\n",
    "    unique_values = df[column_name].unique()\n",
    "    nan_count = df[column_name].isna().sum()\n",
    "    total_count = df.shape[0]\n",
    "    nan_percentage = (nan_count / total_count) * 100\n",
    "\n",
    "    print(f\"\\nNumber of NaN values in the {column_name} column is {nan_count} out of {total_count}:\")\n",
    "    print(f\"{nan_percentage:.2f}%\")\n",
    "\n",
    "for e in new_df.columns:\n",
    "    analyze_column(new_df, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32650d19-42ca-4260-be51-dae03fdf1ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085b9cb6-2256-4d19-aa76-b636c36534da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb77b553-ec25-416a-864d-434e28bb24b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
